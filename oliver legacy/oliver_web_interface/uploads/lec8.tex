% \begin{Lecture}{Input/Output}

% \begin{frame}[fragile]
% \Title{Input/Output}
% \begin{itemize}
% 	\item Readings: Chapter 1.4, 5.2.

% 	\item Our interests: 
%         \begin{itemize}
%           \item Concepts rather than numerical details
%           \item Demands on CPU
%           \item Connection of peripherals to CPU
%         \end{itemize}
% \end{itemize}
% \BNotes\ifnum\Notes=1
% \begin{itemize}
% 	\item In the 5th edition, they scattered the IO material through
% 	various parts of the book (thus the scattered readings) and I
% 	think deleted some material.  Possibly we should adjust things
% 	to match, but...
%         \item Also, everything in this section changes very quickly.  I.e., most of the detail in here will be out of date.  Feel free to do web searches to get more up-to-date information.  However, since we're more interested in concepts, it's not so important that some of the material is out of date.
% \end{itemize}
% \fi\ENotes
% \end{frame}

\begin{frame}[fragile]
\Title{Typical Organization of I/O Devices}
\PHFigure{!}{2.5in}{2.5in}{PHALL/F0801}{Typical Organization of Bus}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item You might also want to show
  https://www.intel.com/content/dam/www/program/design/us/en/images/16x9/coffee-lake-refresh-block-diagram-16x9.png.rendition.intel.web.978.550.png

  or some other image of what actually attaches to the CPU/chipset,
  but note that they don't need to know any of the terms on this image
  \item You might also open the device manager on your computer and show them all the devices that share the bus
  \end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{Demands of I/O Systems}
\begin{itemize}
\item Supercomputers: 
\begin{itemize}
\item Single large read at start of batch job
\item ``Checkpointing'' saves during run of job
\end{itemize}
\item Transaction processing: 
\begin{itemize}
\item Many small changes to large database
\item Need good throughput but also reliability
\end{itemize}
\item Unix file systems:
\begin{itemize}
\item Mostly to small files
\item More reading than writing
\end{itemize}
\item Benchmarks have been developed for all three areas
\end{itemize}
\BNotes\ifnum\Notes=1
~% notes text
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{Types and Characteristics of I/O Devices}
\begin{itemize}
\item Characteristics:
\begin{itemize}
\item Behaviour: input, output, storage
\item Partner: human or machine
\item Data rate
\end{itemize}
\item Some types:
\begin{itemize}
\item Keyboard: input device, human partner, data rate 10 bits/sec, 8 - 15 WPM, etc.
\item Graphics display: output device, human partner, \\data rate 60Mb/sec to 500
MB/sec
\item Disk: storage device, machine partner, data rate 100Kb/sec---1500MB/sec
\end{itemize}
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
	\item The 100Kb/sec is for floppy disk drives---probably not relevant
		anymore.
    \item The 60Mb/sec is old monitors; a 3200x1800 display has around
		500MB/sec.  However, this 500MB does not over the IO bus, whereas
		the 60Mb/sec probably did..
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{Keyboards and Mice}
\begin{itemize}
\item Keyboards slow enough that each keystroke can generate interrupt
serviced by CPU
\item Alternate arrangement: bytes generated by keystrokes can be put
into small buffer that is checked periodically by CPU
\item We say the CPU polls the device
\item A mouse needs to be polled more often to ensure smooth animation
of cursor on screen
\item Mouse generates pulses indicating small movements in four
directions (up,down,left,right)
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item On Sun 1's, the mouse was polled by the CPU.  When a CPU bound job
	was running, the mouse got jerky.  On the Sun 2, a separate chip was
	put in to handle the display of the mouse, and all modern machines
	likely do the same.  So the polling comment isn't particulary
	relevant.
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\STitle{Magnetic Disks}
\PHFigure{!}{2.5in}{2.2in}{PHALL/F0804}{Magnetic Disks, Platers, sectors}

Typically 1-4 platters, 10,000-50,000 tracks, 100-500 sectors, 

Pre 2011: 512 bytes per sector.  Since 2011: 4KB per sector.
\BNotes\ifnum\Notes=1
\begin{itemize}
	\item Modern disks have 1---3 platters (possibly double-sided),
		but rarely more due to vibration problems.  Data from
		wikipedia:
\begin{tabular}{lrrrc}
Form factor &	Width 	&Height 	&Largest capacity 	&Platters (Max)\\
\hline
3.5b3 &	102 mm &	25.4 mm &	2 TB[34] (2009) 	&5\\
2.5b3 &	69.9 mm &	7-15 mm &	1 TB[35] (2009) 	&3\\
1.8b3 &	54 mm &		8 mm &		320 GB[36] (2009) 	&3\\
\end{tabular}

From a 2009 product announcement, Samsung has a 1TB drive with 2 platters,
down from 3 platters for an earlier version of the drive (higher density
allowed them to reduce the number of platters).

And in a 2011 product announcement, Seagate (which acquired Samsung's hard drive business in April 2011 for \$1.375 billion), announced a 1TB drive with a single platter, and (potentially) a 3TB drive with 3 platters.
	\item Sectors were mostly 512 bytes from 1956 until 2010.
		In 2010, drives with 4KB sectors began to appear and in
		Jan 2011, 4KB sectors became standard.  
		You can compute the number
		of sectors by simple division.
	\item One 20Gig disk we checked on had 3 platters (6 sides), 
		with 16,000 tracks per platter,
		and 370--800 sectors per track (depending on how close
		to the center you are).
	\item 2011: one report says a ``1TB per platter drive packs
		340,000 tracks across the disk".  Sectors for 1TB platters
		appear to be 4KB now.  A little arithmetic suggests that
		there'll be an average of 735 sectors (4KB sectors) per
		track.
	\item Feb 2014: You could get a 3TB Seagate harddrive for \$101US.
\end{itemize}
\fi\ENotes
\end{frame}

% for comprehensive
\newpage
\begin{frame}[fragile]
\Title{Inside a Magnetic Disk}
\begin{center}
\includegraphics[height=3in]{Figs/Disk-drive.jpg}
\end{center}
% \ifnum\slides=1
% \ifnum\sslides=0
% \begin{center}
% \includegraphics[height=5in]{Figs/Disk-drive.jpg}
% \end{center}
% \else
% Disk drive image
% \fi
% \else
% Disk drive image
% \fi


\BNotes\ifnum\Notes=1
\begin{itemize}
	\item The large silver disk is the platter.  It spins at high speed.
	\item The thing sticking over the platter is the arm; at the tip
		of the arm is the read/write head.  The arm has to move the
		read/write head over the correct track of the disk.
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{Characteristics of Magnetic Disks}
\begin{itemize}
\item Seek time: time to position read/write heads over desired track
\item Rotational latency: must wait for correct sector to rotate under
head
\item In practice these two depend highly on mix of requests
\item Transfer time: time to read block of bits (typically sector) 
\item Can add cache to lower these times
\item RAID: Redundant Array of Inexpensive Disks

	Use redundancy and coding to improve throughput and reliability
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item Seek time of disks (year 2008, from book) are
	0.2 to 2.0 ms (minimum) and 2.9 to 13ms (average).

\item CD seek times appear to be around 100ms, although some drives take
	as long as 300ms for a full seek.  This data is from 2004.  Example:

	MSI DR8-A: random seek 77ms, full seek 160ms

	Pioneer DVR-AD6: random seek 130ms, full seek 296 ms.


\item A 7200 RPM disk completes 120 revolutions per second.

	That means one revolution takes about 8.3~ms.  If the data we want is
	on the track under the head, then on average it takes 4.2~ms until
	the head is positioned to read the data.

	You can get 15,000 RPM disks, but 5400 RPM disks are still in use.

\item You can mention that smart location of files on a disk can reduce seek
	and latency times.  When a disk is full and fragmented, you can no
	longer do smart placement, and performance plummets.

	Tests done at UCBerkeley in the 80's indicated that smart placement
	can be done if the disk is less than 90\% full.  In BSD Unix,
	10\% of the space was left unavailable for user files (although root
	can write to it).
\end{itemize}
\fi\ENotes
%\newpage
\end{frame}

% for comprehensive
\newpage
\begin{frame}[fragile]
\Title{RAID---Redundant Arrays of Inexpensive Disks}
\begin{center}
\Figure{!}{4in}{2.5in}{Figs/raid}
\end{center}

Blue disks marks redundant information.

\BNotes\ifnum\Notes=1
\begin{itemize}
	\item Idea: improve throughput and reliability with multiple disks.
	\item RAID 0: no redunancy.  Not really RAID, but option is
		provided on disks arrays.  Striping allows file to be
		spread across disks and read simultaneously from
		multiple disks.
	\item RAID 1: improve reliability by having data appear on two disks.			Uses half your disks for redundancy.
	\item RAID 2: Uses error detection and correction schemes common
		for memory; not in use on disks any more.
	\item RAID 3: Protection groups: conceptually, use 1 disk out of $n$
		to contain redundant information; basically, the extra disk
		hold the ``sum'' (parity) of what is on the other $n-1$ disks.  
		If one disk fails, can recover data by subtracting remaining
		disks from ``sum'' disk.

		$\sum$ on slides is supposed to indicate that the redundant
		information contain parity (or sum) information about
		the other disks.  So if the other disks are A,B,C,D,E,
		then the $\sum$ disk contains $A+B+C+D+E$.
	\item RAID 4: Similar to RAID 3, but different parity scheme allows
		multiple disk accesses in parallel (in RAID 3, accessing one
		file required accessing all disks).
	\item RAID 5: Problem with RAID 4 is that parity disk is accessed
		for all disk accesses and becomes the bottleneck.  With
		RAID 5, parity information is spread over all disks, removing
		this bottleneck.
	\item RAID 6: Similar to RAID 3--5, but use two disks in each block
		of $n$ to allow for two simultaneous failures rather than one.
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\STitle{Flash Memory/SSD}
\begin{itemize}
	\item HDD may be reaching limits

		15,000 RPM available; 20,000 RPM has problems
	\item Flash memory similar to DRAM:

		put voltage on line and test, but doesn't forget
	\item Four types of cells:

		SLC: 1 bit/cell, 100K writes

		MLC: 2 bits/cell, 10K writes

		TLC: 3 bits/cell, 3K writes

		QLC: 4 bits/cell, 1K writes

\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item At 20,000 RPM, there are vibration and heat issues.  These may or
	may not be solvable.
\item One big advantage of SSD is lack of seek time
\item When you write to a cell, you write either a 1 bit, 2 bit, 3 bit, or 4 bit value.  Ie, it's not that a QLC cell has 4 independent bit, but instead it can take on 16 different values.
\item 2D NAND and 3D NAND technologies affect how many writes the various
		cells have

\item MLC fast enough for consumer SSD
\item Much of the information on SSDs found here came from

\texttt{http://www.anandtech.com/}

some came from

\texttt{http://elitepcbuilding.com/ssd-vs-hdd}
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{Solid State Drive (SSD)}
\begin{itemize}
	\item SSD is groups of flash cells

	Cells are grouped into pages (4KB)

	Pages grouped into blocks (512KB)
	\item Read and write into page
	\item Erase a block (128 pages in a block)
	\item Speed comes from parallelism

		Flash memory: 20MB/s

		SSD: 10 in parallel: 200MB/s
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item The page/block issue is key on next slide; in particular, erasing
	blocks only causes slow down when ``used"
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{SSD Issues}
\begin{itemize}
	\item Characteristics:
	\begin{itemize}
		\item Can read and write pages
		\item Can ONLY erase BLOCKS
		\item Can NOT overwrite page: 

			have to erase first
	\end{itemize}
	\item Result: Controller writes to every page before erasing

	Once ``full'', a 4KB page write requires
	\begin{itemize}
		\item Read 512KB block into temp
		\item Erase 512KB block
		\item Write back valid pages 
	\end{itemize}
	\item Result: SSD fast when new, but later is slow (to write)

		OS writes lots of temp files, so OS appears to slow down

		(OS has properly handled this since Windows 7.)
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item Standard MLC can only be erased 10,000 times before going bad

	Estimates are that with continuous writing, this would take 40 years,
	and that the material itself would last 228 years.
\item Controller keeps track of valid and invalid pages.  On a ``delete"
	a page is merely marked as invalid.  Thus, for a heavily used
	computer, your SSD may be only 50\% full, but the SSD is full
	of valid and invalid pages.

\item The result of writing a 4KB page to a ``full'' SSD 
	is upto 70\% slower on
	some SSD drives than a write to a new SSD.
\item Don't linger on the last bulleted list---it's illustrated on the
	next slide.

	However, what "handled" means is that the the OS erases blocks when
	the drive isn't being used.
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{SSD Write Example}
Starting with 123 pages full and 5 empty...
\Figure{!}{4in}{2.7in}{Figs/ssd-issue1}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item We start with a 512KB block with 123 full pages.  Those are the
	gray ones on the left.

	The 5 white pages are empty.
\item We do a couple of writes, a delete and another write.

	When we delete, the file isn't deleted, it's just marked as invalid.

	So when we do the third write, we have to erase the entire block,
	copy the old stuff back to the block (the dark lines indicate the
	pages that are copied) and then write the new data.
\end{itemize}
\fi\ENotes
\end{frame}

\iffalse
\begin{frame}[fragile]
\Title{TRIM}
\begin{itemize}
\item TRIM does the obvious: on delete, tell SSD to erase and rewrite
\item 2009: TRIM beta works well unless you put your system to sleep...
\item TRIM needs OS support

	Windows 7 supports it

	Vista, XP do not.
\item Wiper tools available for Windows 7, Vista, XP to restore SSD to ``new"
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item In one TRIM beta implementation, putting your computer to sleep
	could result in lost data.
\item Once bugs worked out of TRIM, SSDs will likely wipe HDD off the map.

	Initially only use where speed is important (because of size issues),
	but assuming size get bigger and prices come down, the speed issue
	is almost a pure win for SSDs.

	Potentially, HDD will improve (they have in the past), but as
	mentioned earlier, they may be reaching physical limits.

	Fall 2011: Two years after the above was written, the price
	difference between SSD and HDD continues to widen (with HDD
	being cheaper), and while HDD continue to get bigger, SSD's
	haven't increased in size significantly.

\item HDDs may remain where many short term files result in lots of
	read/writes and the 10,000 limit is quickly reached.
\end{itemize}
\fi\ENotes
\end{frame}
\fi

\begin{frame}[fragile]
\Title{Comparison (Fall 2011 vs Fall 2013 vs Fall 2016)}
{\small
\begin{tabular}{c|ccc|ccc|ccc}
&\multicolumn{3}{c|}{2011}
&\multicolumn{3}{c|}{2013}
&\multicolumn{3}{c}{2016}\\
\hline
&Size & Price & \$/TB& Size & Price & \$/TB& Size & Price & \$/TB\\
\hline
HDD & 3TB & \$180 & \$60 & 4TB & \$200 & \$50 & 8TB & \$320 & \$40 \\
SSD & 250GB & \$400 & \$1600 & 1TB & \$570 & \$570 & 2TB & \$650 & \$325 \\
\end{tabular}
}

\begin{itemize}
\item Observations:
\begin{itemize}
	\item SSDs increasing in capacity quicker than HDDs
	\item SSDs price per unit storage dropping faster than HDDs
\end{itemize}
\item Speed:

	SSD range from 100 times slower to 10 times faster

	(newer SSD's may not be slower than HDD any more)

\item Hybrid models 

	HDD with 100GB SSD used as cache: HHD
\item SSDs are appearing in large data centers (Amazon, Facebook, Dropbox)
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item The speed range is dependent on the operation performed.  HDD's work
	great when streaming large files, but are terrible at random
	access.

\item Size range matters for SSD because of large price, performance range

\item Space per unit cost doubled every 14 months from 1980--2011.
	Then floods, etc., were blamed for an increase in price, so
	prices now are higher than listed on this page, but (2013) are
	getting back down to what they were in 2011.
\item One surprise over the past two years is that HHDs haven't increased
	SSD cache size (i.e., in 2011 they had 100GB SSDs, and in 2013 they
	had 100GB SSDs).  A larger cache should improve performance.
\item 2016 HDD: the 8TB was archival (ie, poor performance).  Better 
	performance cost about \$100/TB for 8TB.
\end{itemize}
\fi\ENotes
\end{frame}


\begin{frame}[fragile]
\STitle{Networks}
\begin{itemize}
\item Early Ethernet: one-wire bus,  no central control

Nodes listen for silence, then transmit

Collisions resolved by random backoff

\item Current Ethernet (twisted pair): Star network

	All computers talk to central hub
\item Basis for most LANs (local area networks) when combined with
some switching
\item Long-haul networks: distances of 10 to 10,000 km
\item Usually packet-switched: information broken up, reassembled
\item Protocols for reliable delivery (TCP) and addressing (IP)
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
	\item iPv4 was original protocol and had 32-bit addresses.  Seemed
		like enough when there were only 2 computers on the network.

		ip4 officially ran out of addresses on Feb 3, 2011.

		ip6 (128 bits) started well before 2011 (China was one of the first
		to switch because it needed the addresses), and the transition 
		to ip6 appears to have been tranparent to most of us.
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\STitle{Buses}
\PHFigure{!}{3in}{2.5in}{PHALL/F0807}{Direct memory mapping}

A typical output operation (defined from processor point of view)
\BNotes\ifnum\Notes=1
In the top illustration, the processor initiates the output operation
by signalling memory using the control lines and transferring an
address using the data lines. In the middle illustration, memory is
responding; the bus is quiet. In the bottom illustration, memory is
signalling the disk via the control lines that it is transferring the
required data on the data lines.
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{Bus Parameters}
\begin{itemize}
\item Buses are typical bottleneck for I/O throughput
\item Limitations on speed: length, number of devices
%\item Types: processor-memory, I/O, backplane
\item Communication schemes: synchronous, asynchronous
\begin{itemize}
\item Synchronous have clock connected to control lines, fixed
protocols

	 Fast, but require similar devices and must be short
\item Asynchronous buses require 
``handshaking'' communication protocol

Handled as two communicating FSMs
\end{itemize}
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
%\item Processor-memory buses are short, high-speed, and designed so as
%to maximize bandwidth; they are usually proprietary, and at design
%time the complete system is known.
%\item I/O buses are long and have heterogeneous devices on them; the
%complete system is usually unknown at design time.
%\item Backplane buses mix the two functions, and are often
%standardized. 
%\item The differences between these may be minor.
\item Devices on synchronous buses must run at the same clock rate,
and longer buses lead to clock skew.
\item There is an example of a handshaking protocol in the book on
page 661 involving three control lines: ReadReq, Ack, and DataRdy.
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{Bus Arbitration}
\begin{itemize}
\item Who decides who gets access to a shared bus?
\item Simplest scheme: the processor
\item Better: have multiple bus masters
\item Choosing among bus masters is bus arbitration
\item Need fairness but also scheme of priorities
\item Typical schemes: daisy-chain, centralized, token-passing,
  self-selection, collision detection (as in Ethernet)
  \medskip
\item Note: central switch rather than shared lines is also used
\end{itemize}
\BNotes\ifnum\Notes=1
~% notes text
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{Interfacing with Processor and Memory}
\begin{itemize}
\item Two schemes for processor I/O: memory-mapped, special instructions
\item Polling versus interrupt-driven I/O
\item Usually need to have different levels of interrupts depending on
priority
\item DMA: direct memory access (device controller transfers data
right into memory without processor involvement)
\item Creates problems with caches or VM
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item Memory-mapped I/O: reads/writes to certain addresses interpreted as
commands to devices
\item Using special instructions can ensure safety by only allowing OS
to execute them; with memory-mapped, have to use memory protection schemes.
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\STitle{Conclusion}
 \underline{\textbf{Lecture Summary}}
 \begin{itemize}
 \item Connection of peripherals to CPU
          \item Demands on CPU
          \item Broader concepts 
          % rather than numerical details       
\end{itemize}

 \underline{\textbf{Assigned Readings}}
\begin{itemize}
     % \item \textbf{Read} Section 5.5, 5.6 and 5.8
   \item \textbf{Read} Chapter 1.4 and 5.2.
     \end{itemize}
    \underline{\textbf{Next Steps}}
    \begin{itemize}
     \item \textbf{Review} Solid State Drives (SSD), Hard Disk Drives (HDD)
\begin{itemize}
    \item Finish the exercises in A7, coverage material are lectures from Module 6 - Memory Hierarchy
\end{itemize}
% \item Next class, we will discuss the begin discussing Input/Output (I/O) Devices. 
% \item \textbf{Attempt} questions in next week's tutorial. 
    \item \textbf{Ask} questions in office hours or the next tutorial.
 \end{itemize}

\end{frame}

 \begin{frame}{Additional Slides}
     Remaining slides are additional notes for your information.
 \end{frame}

\iffalse
% This is outdated
\begin{frame}[fragile]
\Title{Example: Pentium Bus}
\Figure{!}{6in}{3in}{Figs/pentiumBus}
\BNotes\ifnum\Notes=1
\begin{itemize}
	\item The figure is a rough sketch of a Pentium bus.  
		The blue lines are the busses; the thickness is a rough
		indication of the bandwidth of each bus.
	\item The CPUs are connected to their cache with a {\it backside} bus.
	
	\item The frontside bus (the System Bus)
		is usually multiple words wide, allowing
		for faster transfer of block of words from memory into
		cache.

	\item The {\it North Bridge} connects the System Bus to the
		remaining busses.  Often the graphics card will use
		store textures (data) in the computer's RAM and needs
		fast, direct access to it.

	\item The {\it South Bridge} connects the remaining busses to
		the North Bridge.  In particular, these other busses
		are significantly slower than the ones connected to
		the North Bridge.

		Some details are omitted; e.g., the SCSI interface connects
		to a SCSI bus that connects to other equipment.

	\item This material came from

		Computer Architecture and Organization

		Murdocca and Heuring
\end{itemize}
\fi\ENotes
\end{frame}
\fi

% \end{Lecture}

\iffalse
\begin{Lecture}{Starting Your Computer}
\begin{frame}[fragile]
\Title{What happens when you turn on your computer?}

SoC: System on Chip

\Figure{!}{4in}{2.5in}{Figs/SoC}
\begin{itemize}
	\item High speed synchronous bus (100MHz) connecting
	             CPU, RAM, ROM, Ethernet, USB, Bridge

\item 				 	Bridge connects to peripheral bus: Asynchronous, 50MHz, hand shaking
					     	       protocol
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item This is based on Bill Cowan's lecture on the topic in the Real Time course

\item The basic idea is: there is a System on Chip that's on the mother board.

		When the computer starts, the SoC boots first using a program in the
		ROM.  

		This program eventually loads the boot code found on the mother board.

		The program on the mother board then reboots (using its code rather
						than the ROM code).
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{Power On}
\begin{itemize}
\item Power to SoC: 30ms to get stable
\item  Clock needs time to get started

	   Power supply generates reset for 0.5 seconds

\item Reset

		Reset from power supply goes to CPU

\item CPU sends reset signal to other devices on SoC

	Except RAM (for debugging)
\item On Reset:
\begin{itemize}
\item PC $\leftarrow$ 0x0000 0000

 PC Address aliased to ROM (not RAM)

\end{itemize}
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
\item For desktop computers, house power operates at 60Hz, and can be used
	for the clock to count the half a second; not sure what happends on
	laptop.
\item The PC Address aliasing is handled by a Crossbar switch, which sends
	the PC address to either ROM or RAM.
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\Title{PC starting at 0}
Executing ROM code (SoC boot code):
\begin{enumerate}
		\item Disable watchdog

	   Watchdog: reset processor if it reaches end of count

		\item Configure CPU
		\begin{itemize}
		    \item turn off cache
		   	\item turn off memory manager
		    \item turn off branch predictor
		\end{itemize}
		   ie, get to known state

		\item Configure memory controller

		\item Load code into static RAM (flash, bus)

		\item Clear boot mode
			   
				Execute from RAM rather than ROM

		\item Start executing flash boot code loaded in 4th step
\end{enumerate}
\BNotes\ifnum\Notes=1
~
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\begin{itemize}
	\item Flash boot code

		 Does all of the above again

		 (board manufacturer knows what's on board vs SoC)
	\item Once configured for board, OS loaded and OS execution begins
\end{itemize}
\BNotes\ifnum\Notes=1
\begin{itemize}
	\item The SoC is just one part of what's on the board.  The SoC 
		manufacturer doesn't know what's on the board, so after an initial
		start up to load the board code, the board boot code (which does
        know what's on the board) starts from
		scratch to use all components on board effectively.
\end{itemize}
\fi\ENotes
\end{frame}

\begin{frame}[fragile]
\STitle{Conclusion}
 \underline{\textbf{Lecture Summary}}
 \begin{itemize}
\item Summarize benefits of Virtual Memory:
\begin{itemize}
  \item Program address space is larger than physical memory
  \item Multiple programs share memory (RAM)
\end{itemize}
\item Implement Virtual Memory
  \begin{itemize}
  \item Map program address (virtual address to RAM address (physical address
  \item Map blocks (pages) [4KB $\Rightarrow$ 12-bit sub-address]
  \item \hl{Page table contains this mapping}
  \item Store copy of what's in RAM on disk (swap space)
  \item Use reference bits (occasionally cleared) to approximate LRU for replacing pages in memory
  \end{itemize}
\end{itemize}

 \underline{\textbf{Assigned Readings}}
\begin{itemize}
     % \item \textbf{Read} Section 5.5, 5.6 and 5.8
   \item \textbf{Read} Section 5.7
     \end{itemize}
    \underline{\textbf{Next Steps}}
    \begin{itemize}
     \item \textbf{Review} Cache, TLB, Page tables
\begin{itemize}
    \item Start the exercises in A7
\end{itemize}
\item Next class, we will discuss the begin discussing Input/Output (I/O) Devices. 
\item \textbf{Attempt} questions in next week's tutorial. 
    \item \textbf{Ask} questions in office hours or the next tutorial.
 \end{itemize}

\end{frame}

 \begin{frame}{Additional Slides}
     Remaining slides are additional notes for your information.
 \end{frame}


\end{Lecture}
\fi
