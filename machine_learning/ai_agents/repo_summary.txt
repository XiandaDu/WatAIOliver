�� Codebase Summary (Sun Aug 31 22:28:37 EDT 2025)
===================================


🔹 FILE: ./config.py
-----------------------------------
"""
Configuration for Speculative AI Multi-Agent System
"""

from typing import Dict, Optional
from pydantic import BaseModel, Field


class SpeculativeAIConfig(BaseModel):
    """Simplified configuration for the Speculative AI system"""
    
    # Core system settings
    max_debate_rounds: int = Field(default=3, description="Maximum number of debate iterations")
    convergence_threshold: float = Field(default=0.7, description="Score threshold for debate convergence")
    
    # Retrieval settings
    retrieval_k: int = Field(default=10, description="Number of documents to retrieve")
    
    # Agent temperatures (only what we actually use)
    strategist_temperature: float = Field(default=0.8, description="Creativity level for strategist")
    critic_temperature: float = Field(default=0.1, description="Conservative temperature for critic")
    
    # Debug
    enable_debug_logging: bool = Field(default=True, description="Enable debug output")
    
    class Config:
        extra = "forbid"  # Prevent unused config options 

🔹 FILE: ./app/main.py
-----------------------------------
"""
Speculative AI FastAPI Service

Provides REST API endpoints for the multi-agent speculative AI system.
"""

import logging
import sys
import os
from typing import Optional, Dict, Any
from contextlib import asynccontextmanager 
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field

from ai_agents.orchestrator import MultiAgentOrchestrator
from ai_agents.config import SpeculativeAIConfig
from rag_system.services.rag_service import RAGService
from rag_system.llm_clients.gemini_client import GeminiClient
from rag_system.llm_clients.cerebras_client import CerebrasClient
from rag_system.app.config import get_settings

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Suppress noisy external library logs
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("google").setLevel(logging.WARNING)
logging.getLogger("google_genai").setLevel(logging.WARNING)
logging.getLogger("supabase").setLevel(logging.WARNING)
logging.getLogger("urllib3").setLevel(logging.WARNING)

logger = logging.getLogger("ai_agents.service")


# Global service instances
_orchestrator: Optional[MultiAgentOrchestrator] = None
_initialization_error: Optional[str] = None


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Manage service lifecycle"""
    global _orchestrator, _initialization_error
    
    try:
        logger.info("Initializing Multi-Agent System Service...")
        
        # Initialize dependencies
        rag_settings = get_settings()
        rag_service = RAGService(rag_settings)
        llm_client = CerebrasClient(
            api_key=rag_settings.cerebras_api_key,
            model="qwen-3-235b-a22b-instruct-2507",
        )
        
        # Initialize orchestrator
        config = SpeculativeAIConfig()
        _orchestrator = MultiAgentOrchestrator(
            config=config,
            rag_service=rag_service,
            llm_client=llm_client,
            logger=logger
        )
        
        logger.info("Multi-Agent System Service initialized successfully")
        
    except Exception as e:
        _initialization_error = f"Service initialization failed: {str(e)}"
        logger.error(_initialization_error)
    
    yield
    
    logger.info("Multi-Agent System Service shutting down")


# Create FastAPI app
app = FastAPI(
    title="Multi-Agent System",
    description="Advanced RAG with multi-agent reasoning and debate-based verification",
    version="1.0.0",
    docs_url="/docs",
    lifespan=lifespan
)


# Request/Response Models
class QueryRequest(BaseModel):
    """Request model for multi-agent system queries"""
    query: str = Field(..., description="User's question or query")
    course_id: str = Field(..., description="Course identifier for context retrieval")
    session_id: Optional[str] = Field(default=None, description="Optional session identifier")
    metadata: Optional[Dict[str, Any]] = Field(default=None, description="Additional metadata")
    heavy_model: Optional[str] = Field(default=None, description="Optional heavy model for debate agents")
    course_prompt: Optional[str] = Field(default=None, description="Course-specific system prompt")
    config_overrides: Optional[Dict[str, Any]] = Field(default=None, description="Configuration overrides")


class QueryResponse(BaseModel):
    """Response model for multi-agent system queries"""
    success: bool
    answer: Optional[Dict[str, Any]] = None
    metadata: Optional[Dict[str, Any]] = None
    debug_info: Optional[Dict[str, Any]] = None
    error: Optional[Dict[str, Any]] = None


class SystemStatusResponse(BaseModel):
    """Response model for system status"""
    status: str
    orchestrator_status: str
    agents_initialized: int
    configuration: Dict[str, Any]
    execution_stats: Dict[str, Any]


def get_orchestrator() -> MultiAgentOrchestrator:
    """Get orchestrator instance with error handling"""
    global _orchestrator, _initialization_error
    
    if _initialization_error:
        raise HTTPException(
            status_code=503,
            detail=f"Multi-Agent System service unavailable: {_initialization_error}"
        )
    
    if _orchestrator is None:
        raise HTTPException(
            status_code=503,
            detail="Multi-Agent System orchestrator not initialized"
        )
    
    return _orchestrator


# API Endpoints
@app.get("/")
async def root():
    """Root endpoint with service information"""
    return {
        "message": "Speculative AI Multi-Agent System",
        "version": "1.0.0",
        "description": "Advanced RAG with multi-agent reasoning and debate-based verification",
        "endpoints": {
            "process_query": "/query",
            "system_status": "/status",
            "health": "/health",
            "docs": "/docs"
        }
    }


@app.post("/test")
async def test_agents():
    """Test endpoint to debug agent execution"""
    try:
        orchestrator = get_orchestrator()
        
        # Test each agent individually
        from ai_agents.agents.base_agent import AgentInput
        
        test_input = AgentInput(
            query="test query",
            context=[],
            metadata={"course_id": "test_course", "test": True},
            session_id="test"
        )
        
        results = {}
        
        # Test retrieve agent
        try:
            retrieve_result = await orchestrator.retrieve_agent.execute(test_input)
            results["retrieve"] = {"success": retrieve_result.success, "error": retrieve_result.error_message}
        except Exception as e:
            results["retrieve"] = {"success": False, "error": str(e)}
        
        return {
            "test_results": results,
            "orchestrator_status": "operational"
        }
        
    except Exception as e:
        return {
            "error": f"Test failed: {str(e)}",
            "orchestrator_status": "failed"
        }


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    try:
        orchestrator = get_orchestrator()
        system_status = orchestrator.get_system_status()
        
        return {
            "status": "healthy",
            "timestamp": "2024-01-01T00:00:00Z",  # Would use datetime.now() in production
            "agents_operational": system_status["agents_initialized"],
            "service": "multi_agent_system"
        }
        
    except Exception as e:
        return {
            "status": "unhealthy",
            "error": str(e),
            "service": "multi_agent_system"
        }


@app.get("/status", response_model=SystemStatusResponse)
async def get_system_status():
    """Get comprehensive system status"""
    try:
        orchestrator = get_orchestrator()
        status = orchestrator.get_system_status()
        
        return SystemStatusResponse(
            status="operational",
            **status
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Status check failed: {str(e)}")


@app.post("/query", response_model=QueryResponse)
async def process_query(request: QueryRequest):
    """
    Process a query through the speculative AI system
    
    This endpoint orchestrates the complete multi-agent workflow:
    1. Enhanced retrieval with query reframing
    2. Multi-round debate between Strategist and Critic
    3. Moderator-controlled convergence
    4. Final answer synthesis by Reporter
    """
    try:
        orchestrator = get_orchestrator()
        
        # Generate session ID if not provided
        import uuid
        session_id = request.session_id or str(uuid.uuid4())[:8]
        
        # Apply configuration overrides if provided
        if request.config_overrides:
            logger.info(f"Applying config overrides: {request.config_overrides}")
            # Note: In production, you'd create a new config with overrides
            # For now, we'll log the request but use default config
        
        # Process the query and collect the final result
        final_result = None
        async for chunk in orchestrator.process_query(
            query=request.query,
            course_id=request.course_id,
            session_id=session_id,
            metadata=request.metadata or {},
            heavy_model=request.heavy_model,
            course_prompt=request.course_prompt,
        ):
            if chunk.get("status") == "complete":
                final_result = chunk.get("final_response")
                break # Stop iterating after receiving the final response
            elif chunk.get("error"):
                # If an error chunk is yielded, raise an HTTPException immediately
                raise HTTPException(status_code=500, detail=f"Agent processing error: {chunk['error']['message']}")
        
        if not final_result:
            raise HTTPException(status_code=500, detail="Agent system did not return a final response.")
        
        return QueryResponse(**final_result)
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Query processing failed: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Query processing failed: {str(e)}")


@app.post("/reset-metrics")
async def reset_system_metrics():
    """Reset system performance metrics"""
    try:
        orchestrator = get_orchestrator()
        orchestrator.reset_system_metrics()
        
        return {
            "success": True,
            "message": "System metrics reset successfully"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Metrics reset failed: {str(e)}")


# Health check for specific components
@app.get("/health/agents")
async def agents_health_check():
    """Check health of individual agents"""
    try:
        orchestrator = get_orchestrator()
        system_status = orchestrator.get_system_status()
        
        return {
            "agents_registered": system_status["agents_registered"],
            "agent_metrics": system_status["agent_metrics"],
            "all_agents_operational": system_status["agents_registered"] == 5  # Expected number of agents
        }
        
    except Exception as e:
        return {
            "error": str(e),
            "agents_operational": False
        }


# Development/debugging endpoints (only in debug mode)
@app.get("/debug/config")
async def get_debug_config():
    """Get current configuration (debug only)"""
    try:
        orchestrator = get_orchestrator()
        
        if not orchestrator.config.enable_debug_logging:
            raise HTTPException(status_code=403, detail="Debug mode not enabled")
        
        return {
            "config": {
                "max_debate_rounds": orchestrator.config.max_debate_rounds,
                "retrieval_k": orchestrator.config.retrieval_k,
                "speculation_rounds": orchestrator.config.speculation_rounds,
                "enable_debug_logging": orchestrator.config.enable_debug_logging,
                "model_routing": {k: v.value for k, v in orchestrator.config.model_routing.items()}
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))


if __name__ == "__main__":
    import uvicorn
    from backend.constants import ServiceConfig
    
    uvicorn.run(
        "main:app",
        host=ServiceConfig.DEFAULT_HOST,
        port=ServiceConfig.AGENTS_SYSTEM_PORT,
        reload=True,
        log_level="info"
    ) 

🔹 FILE: ./script.sh
-----------------------------------
#!/bin/bash

# Output file for the summary
OUTPUT="repo_summary.txt"

# Write header with timestamp to output file (overwrite if exists)
echo "�� Codebase Summary ($(date))" > "$OUTPUT"
echo "===================================" >> "$OUTPUT"

# Find files with specified extensions, excluding certain directories,
# and process each file to add a preview of its content to the summary
find . -type f \( \
    -name "*.py" -o -name "*.go" -o -name "*.js" -o -name "*.ts" -o \
    -name "*.yaml" -o -name "*.sh" -o -name "*.html" -o -name "*.md" \
\) \
! -path "*/node_modules/*" \
! -path "*/.git/*" \
! -path "*/__pycache__/*" \
! -path "./sh/*" | while read -r file; do
    echo -e "\n\n🔹 FILE: $file" >> "$OUTPUT"
    echo "-----------------------------------" >> "$OUTPUT"
    cat "$file" >> "$OUTPUT"
done

echo -e "\n\n✅ Done. Summary saved to $OUTPUT"



🔹 FILE: ./__init__.py
-----------------------------------
# Empty __init__.py to maintain package structure 

🔹 FILE: ./agents/strategist_agent.py
-----------------------------------
"""
Strategist Agent - Strategy Proposer

This agent generates well-structured, insightful initial draft solutions with detailed 
Chain-of-Thought (CoT) reasoning based on retrieved context.
"""

from typing import List, Dict, Any
from ai_agents.agents.base_agent import BaseAgent, AgentInput, AgentOutput, AgentRole


class StrategistAgent(BaseAgent):
    """
    Strategist Agent - Strategy Proposer
    
    Responsible for:
    1. Analyzing retrieved context and user query
    2. Generating step-by-step Chain-of-Thought (CoT)
    3. Producing an initial draft solution
    4. Encouraging divergent thinking and exploration
    """
    
    def __init__(self, config, llm_client=None, logger=None):
        super().__init__(AgentRole.STRATEGIST, config, llm_client, logger)
        
        # Strategist-specific prompts
        self.system_prompt = self._build_system_prompt()
        
    def _build_system_prompt(self) -> str:
        """Build the system prompt for the strategist"""
        return """
        You are an expert academic strategist and problem-solving assistant. Your role is to:

        1. ANALYZE the provided context and question thoroughly
        2. GENERATE a detailed Chain-of-Thought (CoT) breaking down your approach
        3. PRODUCE a comprehensive draft solution

        Key principles:
        - Think step-by-step and show your reasoning process
        - Use the provided context as your primary source of truth
        - Be creative and explore multiple solution paths when appropriate
        - Focus on educational value and clarity
        - Don't aim for perfection - this is a draft for further refinement
        - Take a moment to reflect deeply before responding

        Your output should be structured and detailed, suitable for critical review.
        """
    
    async def process(self, agent_input: AgentInput) -> AgentOutput:
        """
        Generate draft solution with Chain-of-Thought reasoning
        
        Args:
            agent_input: Contains query, retrieved context, and metadata
            
        Returns:
            AgentOutput: Draft solution with CoT and metadata
        """
        try:
            query = agent_input.query
            context = agent_input.context
            
            self.logger.info(f"Strategist analyzing query: '{query[:100]}...'")
            self.logger.info(f"Working with {len(context)} context items")
            
            # Build the complete prompt with context and course-specific guidance
            metadata = agent_input.metadata or {}
            strategist_prompt = self._build_strategist_prompt(query, context, metadata)
            
            # Generate the draft with CoT
            response = await self._generate_draft_solution(strategist_prompt)
            
            if not response:
                raise Exception("Failed to generate draft solution from LLM")
            
            # Parse the response into CoT and draft
            parsed_response = self._parse_llm_response(response)
            
            # Validate the output quality
            quality_score = self._assess_draft_quality(parsed_response, context)
            
            return AgentOutput(
                success=True,
                content={
                    "draft_id": f"draft_{agent_input.session_id[:8]}",
                    "draft_content": parsed_response["draft_content"],
                    "chain_of_thought": parsed_response["chain_of_thought"],
                    "quality_assessment": {
                        "score": quality_score,
                        "reasoning_steps": len(parsed_response["chain_of_thought"]),
                        "context_utilization": parsed_response["context_references"]
                    }
                },
                metadata={
                    "query": query,
                    "context_count": len(context),
                    "temperature_used": self.get_temperature(),
                    "strategy": "creative_exploration"
                },
                processing_time=0.0,  # Set by parent class
                agent_role=self.agent_role
            )
            
        except Exception as e:
            self.logger.error(f"Strategist failed: {str(e)}")
            raise e
    
    def _build_strategist_prompt(self, query: str, context: List[Dict[str, Any]], metadata: Dict[str, Any] = None) -> str:
        """Build comprehensive prompt with context and query"""
        
        # Format context information
        context_text = self._format_context_for_prompt(context)
        
        # Get course-specific prompt if available
        course_prompt = metadata.get('course_prompt') if metadata else None
        system_guidance = course_prompt or "You are a helpful educational assistant."
        
        # Check for previous feedback from Critic (for revision rounds)
        previous_feedback = metadata.get('previous_feedback') if metadata else None
        round_num = metadata.get('round', 1) if metadata else 1
        
        # Build revision instructions if this is a follow-up round
        revision_section = ""
        if previous_feedback and round_num > 1:
            revision_section = f"""
        
️  CRITICAL: REVISION ROUND {round_num}
        Your previous draft had issues that need correction. The Critic found:
        
        FEEDBACK FROM PREVIOUS ROUND:
        {previous_feedback}
        
REQUIRED ACTION: 
        You MUST address these specific issues in your new draft. Don't just repeat the same content - 
        actively fix the logical flaws, factual errors, and missing details identified above.
        """
        
        prompt = f"""
        {self.system_prompt}
        
        COURSE-SPECIFIC GUIDANCE:
        {system_guidance}
        {revision_section}
        
        CONTEXT INFORMATION:
        {context_text}
        
        USER QUERY:
        {query}
        
        Please provide your response in the following structured format:
        
        ## CHAIN OF THOUGHT
        
        Step 1: [Your first reasoning step]
        - [Detailed explanation of this step]
        - [Why this step is necessary]
        
        Step 2: [Your second reasoning step]
        - [Detailed explanation]
        - [Connection to previous step]
        
        [Continue with additional steps as needed]
        
        ## DRAFT SOLUTION
        
        [Your comprehensive draft answer to the query, incorporating insights from your Chain of Thought and the provided context]
        
        ## CONTEXT REFERENCES
        
        [List the specific context items you referenced and how they informed your solution]
        
        Remember: This is a draft meant for critical review. Focus on clear reasoning and thorough analysis rather than perfect polish.
        """
        
        return prompt
    
    def _format_context_for_prompt(self, context: List[Dict[str, Any]]) -> str:
        """Format retrieved context for inclusion in prompt"""
        if not context:
            return "No additional context provided."
        
        formatted_contexts = []
        
        for i, ctx_item in enumerate(context[:8]):  # Limit to prevent prompt overflow
            text = ctx_item.get('text', ctx_item.get('content', ''))
            score = ctx_item.get('score', 'N/A')
            source = ctx_item.get('source', {})
            
            formatted_context = f"""
=== CONTEXT SOURCE {i+1} (Relevance: {score}) ===
{text}
=== END CONTEXT SOURCE {i+1} ===
Source: {source}
            """
            
            formatted_contexts.append(formatted_context)
        
        return "\n".join(formatted_contexts)
    
    async def _generate_draft_solution(self, prompt: str) -> str:
        """Generate draft solution using LLM"""
        try:
            if not self.llm_client:
                raise Exception("No LLM client available")
            
            temperature = self.get_temperature()
            self.logger.info("=" * 250)
            self.logger.info("STRATEGIST LLM GENERATION")
            self.logger.info("=" * 250)
            self.logger.info(f"Temperature: {temperature}")
            self.logger.info("-" * 250)
            self.logger.info("PROMPT:")
            self.logger.info(prompt)
            self.logger.info("-" * 250)
            
            # Call LLM with creative temperature for exploration
            response = await self._call_llm(prompt, temperature)
            
            self.logger.info("LLM RESPONSE:")
            self.logger.info(response)
            self.logger.info("=" * 250)
            
            if len(response) < 100:  # Sanity check for reasonable response length
                raise Exception(f"Response too short ({len(response)} chars), likely generation failure")
            
            return response
            
        except Exception as e:
            self.logger.error(f"Draft generation failed: {str(e)}")
            raise e
    
    def _parse_llm_response(self, response: str) -> Dict[str, Any]:
        """Parse LLM response into structured components"""
        
        result = {
            "draft_content": "",
            "chain_of_thought": [],
            "context_references": 0
        }
        
        self.logger.info("=" * 250)
        self.logger.info("STRATEGIST RESPONSE PARSING")
        self.logger.info("=" * 250)
        
        try:
            # Split response into sections
            sections = {}
            current_section = None
            current_content = []
            
            for line in response.split('\n'):
                line = line.strip()
                
                if line.startswith('## '):
                    # Save previous section
                    if current_section:
                        sections[current_section] = '\n'.join(current_content)
                    
                    # Start new section
                    current_section = line[3:].strip().lower()
                    current_content = []
                    
                elif current_section and line:
                    current_content.append(line)
            
            # Save last section
            if current_section:
                sections[current_section] = '\n'.join(current_content)
            
            self.logger.info(f"PARSED SECTIONS: {list(sections.keys())}")
            
            # Extract Chain of Thought
            cot_text = sections.get('chain of thought', '')
            self.logger.info("-" * 250)
            self.logger.info("CHAIN OF THOUGHT RAW TEXT:")
            self.logger.info(f"'{cot_text}'")
            self.logger.info("-" * 250)
            
            result["chain_of_thought"] = self._parse_chain_of_thought(cot_text)
            
            self.logger.info("PARSED COT STEPS:")
            for i, step in enumerate(result["chain_of_thought"]):
                self.logger.info(f"  Step {i+1}: {step}")
            self.logger.info("-" * 250)
            
            # Extract Draft Solution
            result["draft_content"] = sections.get('draft solution', response)  # Fallback to full response
            
            # Count context references
            context_refs = sections.get('context references', '')
            result["context_references"] = len([line for line in context_refs.split('\n') if 'context' in line.lower()])
            
        except Exception as e:
            self.logger.warning(f"️ Response parsing failed, using raw response: {str(e)}")
            
            # Fallback: use raw response as draft
            result["draft_content"] = response
            result["chain_of_thought"] = [{"step": 1, "thought": "Raw response provided due to parsing issues"}]
        
        self.logger.info("=" * 250)
        return result
    
    def _parse_chain_of_thought(self, cot_text: str) -> List[Dict[str, Any]]:
        """Parse Chain of Thought text into structured steps"""
        steps = []
        current_step = None
        step_number = 0
        
        for line in cot_text.split('\n'):
            line = line.strip()
            
            if line.startswith('Step '):
                # Save previous step
                if current_step:
                    steps.append(current_step)
                
                # Start new step
                step_number += 1
                step_text = line[line.find(':') + 1:].strip() if ':' in line else line[5:].strip()
                current_step = {
                    "step": step_number,
                    "thought": step_text,
                    "details": []
                }
                
            elif current_step and line.startswith('-'):
                # Add detail to current step
                detail = line[1:].strip()
                current_step["details"].append(detail)
            
            elif current_step and line and not line.startswith('Step'):
                # Add general content to current step
                current_step["thought"] += " " + line
        
        # Save last step
        if current_step:
            steps.append(current_step)
        
        # If no structured steps found, create a single step
        if not steps and cot_text.strip():
            steps.append({
                "step": 1,
                "thought": cot_text.strip(),
                "details": []
            })
        
        return steps
    
    def _assess_draft_quality(self, parsed_response: Dict[str, Any], context: List[Dict[str, Any]]) -> float:
        """Assess the quality of the generated draft"""
        
        score = 0.0
        max_score = 1.0
        
        # Check draft content length and substance
        draft_content = parsed_response.get("draft_content", "")
        if len(draft_content) > 200:
            score += 0.3
        elif len(draft_content) > 100:
            score += 0.15
        
        # Check Chain of Thought quality
        cot_steps = parsed_response.get("chain_of_thought", [])
        if len(cot_steps) >= 3:
            score += 0.3
        elif len(cot_steps) >= 2:
            score += 0.2
        elif len(cot_steps) >= 1:
            score += 0.1
        
        # Check context utilization
        context_refs = parsed_response.get("context_references", 0)
        if context_refs > 0 and context:
            context_utilization = min(context_refs / len(context), 1.0)
            score += 0.4 * context_utilization
        
        self.logger.debug(f"Draft quality assessment: {score:.3f}/{max_score}")
        
        return min(score / max_score, 1.0)
    
    async def _call_llm(self, prompt: str, temperature: float) -> str:
        """Call LLM with error handling"""
        try:
            if hasattr(self.llm_client, 'get_llm_client'):
                llm = self.llm_client.get_llm_client()
                response = await llm.ainvoke(prompt, temperature=temperature)
                return response.content if hasattr(response, 'content') else str(response)
            else:
                # Fallback for different LLM client interfaces
                response = await self.llm_client.generate(prompt, temperature=temperature)
                return str(response)
        except Exception as e:
            self.logger.error(f"LLM call failed: {str(e)}")
            raise e 

🔹 FILE: ./agents/moderator_agent.py
-----------------------------------
"""
Moderator Agent - Debate Flow Controller

This agent evaluates critique reports and decides the next course of action:
convergence, iteration, or deadlock handling.
"""

from typing import List, Dict, Any
from ai_agents.agents.base_agent import BaseAgent, AgentInput, AgentOutput, AgentRole


class ModeratorAgent(BaseAgent):
    """
    Moderator Agent - Arbiter and Debate Flow Controller
    
    Responsible for:
    1. Evaluating critique severity and determining next steps
    2. Managing debate convergence logic
    3. Handling deadlock situations
    4. Generating feedback for iterative improvements
    """
    
    def __init__(self, config, llm_client=None, logger=None):
        super().__init__(AgentRole.MODERATOR, config, llm_client, logger)
        
        # Decision thresholds
        self.convergence_thresholds = {
            "no_issues": 0,
            "low_only": 1,     # Only low severity issues
            "acceptable_minor": 3,  # Up to 3 minor issues
            "critical_limit": 0,    # No critical issues allowed
            "high_limit": 1        # Max 1 high severity issue for convergence
        }
        
    async def process(self, agent_input: AgentInput) -> AgentOutput:
        """
        Evaluate critique report and determine next action
        
        Args:
            agent_input: Contains critique report, draft info, and round count
            
        Returns:
            AgentOutput: Decision on next steps with detailed reasoning
        """
        try:
            # Extract critique data
            critiques = agent_input.metadata.get('critiques', [])
            draft_id = agent_input.metadata.get('draft_id', 'unknown')
            current_round = agent_input.metadata.get('current_round', 1)
            overall_assessment = agent_input.metadata.get('overall_assessment', 'unknown')
            
            self.logger.info(f"️ Moderator evaluating critique report for {draft_id}")
            self.logger.info(f"Round {current_round}, {len(critiques)} issues found")
            
            # ULTRA VERBOSE: Log all received critiques
            self.logger.info(f"\n" + "="*200)
            self.logger.info(f"MODERATOR - DETAILED CRITIQUE ANALYSIS")
            self.logger.info(f"="*200)
            self.logger.info(f"RECEIVED CRITIQUES ({len(critiques)} total):")
            for i, critique in enumerate(critiques):
                severity = critique.get('severity', 'unknown')
                ctype = critique.get('type', 'unknown')
                desc = critique.get('description', 'no description')
                self.logger.info(f"  {i+1}. [{severity.upper()}] {ctype}: {desc}")
            self.logger.info(f"Overall assessment: {overall_assessment}")
            self.logger.info(f"="*200)
            
            # Analyze critique severity
            severity_analysis = self._analyze_critique_severity(critiques)
            
            # Make decision based on rules and thresholds
            decision = self._make_decision(
                severity_analysis, 
                current_round, 
                overall_assessment
            )
            
            # ULTRA VERBOSE: Log severity analysis and decision logic
            self.logger.info(f"\nSEVERITY BREAKDOWN:")
            self.logger.info(f"  CRITICAL: {severity_analysis['by_severity']['critical']}")
            self.logger.info(f"  HIGH: {severity_analysis['by_severity']['high']}")
            self.logger.info(f"  MEDIUM: {severity_analysis['by_severity']['medium']}")
            self.logger.info(f"  LOW: {severity_analysis['by_severity']['low']}")
            self.logger.info(f"  TOTAL: {severity_analysis['total_issues']}")
            
            self.logger.info(f"\nDECISION LOGIC:")
            self.logger.info(f"  Current Round: {current_round}")
            self.logger.info(f"  Convergence Check: {self._check_convergence_conditions(severity_analysis, current_round)}")
            self.logger.info(f"  Action: {decision['action']}")
            self.logger.info(f"  Reasoning: {decision['reasoning']}")
            self.logger.info(f"  Confidence: {decision['confidence']}")
            self.logger.info(f"="*200)
            
            # Generate appropriate response based on decision
            decision_content = await self._generate_decision_content(
                decision, 
                critiques, 
                severity_analysis, 
                current_round
            )
            
            self.logger.info(f"Moderator decision: {decision['action']}")
            
            return AgentOutput(
                success=True,
                content={
                    "decision": decision["action"],
                    "reasoning": decision["reasoning"],
                    "feedback_to_strategist": decision_content.get("feedback", ""),
                    "final_draft": decision_content.get("final_draft"),
                    "critiques": decision_content.get("remaining_critiques"),
                    "decision_metadata": {
                        "round_number": current_round,
                        "severity_breakdown": severity_analysis,
                        "convergence_score": decision["convergence_score"],
                        "decision_confidence": decision["confidence"]
                    }
                },
                metadata={
                    "draft_id": draft_id,
                    "total_critiques": len(critiques),
                    "decision_rationale": decision["detailed_reasoning"]
                },
                processing_time=0.0,  # Set by parent class
                agent_role=self.agent_role
            )
            
        except Exception as e:
            self.logger.error(f"Moderator failed: {str(e)}")
            raise e
    
    def _analyze_critique_severity(self, critiques: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Analyze critique severity distribution and patterns"""
        
        analysis = {
            "total_issues": len(critiques),
            "by_severity": {"critical": 0, "high": 0, "medium": 0, "low": 0},
            "by_type": {"logic_flaw": 0, "fact_contradiction": 0, "hallucination": 0},
            "severity_score": 0.0,
            "most_severe": "none",
            "issue_distribution": []
        }
        
        if not critiques:
            return analysis
        
        # Count by severity and type
        severity_weights = {"critical": 4, "high": 3, "medium": 2, "low": 1}
        total_weight = 0
        
        for critique in critiques:
            severity = critique.get('severity', 'medium')
            issue_type = critique.get('type', 'unknown')
            
            # Update counts
            if severity in analysis["by_severity"]:
                analysis["by_severity"][severity] += 1
            
            if issue_type in analysis["by_type"]:
                analysis["by_type"][issue_type] += 1
            
            # Calculate weighted severity
            weight = severity_weights.get(severity, 2)
            total_weight += weight
            
            # Track most severe issue
            if not analysis["most_severe"] or severity_weights.get(severity, 0) > severity_weights.get(analysis["most_severe"], 0):
                analysis["most_severe"] = severity
        
        # Calculate severity score (0-4 scale)
        analysis["severity_score"] = total_weight / len(critiques) if critiques else 0
        
        # Create distribution summary
        analysis["issue_distribution"] = [
            f"{count} {severity}" for severity, count in analysis["by_severity"].items() if count > 0
        ]
        
        return analysis
    
    def _make_decision(
        self, 
        severity_analysis: Dict[str, Any], 
        current_round: int, 
        overall_assessment: str
    ) -> Dict[str, Any]:
        """
        Make decision based on critique analysis and debate rules
        
        Returns:
            Dict containing decision action, reasoning, and confidence
        """
        
        critical_count = severity_analysis["by_severity"]["critical"]
        high_count = severity_analysis["by_severity"]["high"]
        medium_count = severity_analysis["by_severity"]["medium"]
        total_issues = severity_analysis["total_issues"]
        severity_score = severity_analysis["severity_score"]
        
        # Decision logic based on specified rules
        
        # Rule 1: Check convergence conditions with NEW STRICT criteria
        if self._check_convergence_conditions(severity_analysis, current_round):
            return {
                "action": "converged",
                "reasoning": self._generate_convergence_reasoning(severity_analysis),
                "convergence_score": 1.0 - (severity_score / 4.0),
                "confidence": "high",
                "detailed_reasoning": f"Quality acceptable after {current_round} rounds with {total_issues} remaining issues"
            }
        
        # Rule 2: ESCALATION - 3+ iterations with persistent MEDIUM+ issues
        if current_round >= 3 and (critical_count > 0 or high_count > 0 or medium_count > 0):
            return {
                "action": "escalate_with_warning",
                "reasoning": f"After {current_round} improvement iterations, {critical_count + high_count + medium_count} serious issues remain",
                "convergence_score": 0.4,
                "confidence": "low",
                "detailed_reasoning": f"Persistent quality issues: {critical_count} critical, {high_count} high, {medium_count} medium - escalating to user notification",
                "unresolved_issues": {
                    "critical": critical_count,
                    "high": high_count, 
                    "medium": medium_count
                }
            }
        
        # Rule 3: Standard deadlock (max rounds without escalation case)
        if current_round >= self.config.max_debate_rounds:
            return {
                "action": "abort_deadlock",
                "reasoning": f"Maximum debate rounds ({self.config.max_debate_rounds}) reached",
                "convergence_score": 0.3,
                "confidence": "medium",
                "detailed_reasoning": f"Hard deadlock after {current_round} rounds"
            }
        
        # Rule 3: Critical issues require iteration
        if critical_count > 0:
            return {
                "action": "iterate",
                "reasoning": f"Critical issues ({critical_count}) must be addressed before convergence",
                "convergence_score": 0.1,
                "confidence": "high",
                "detailed_reasoning": "Critical factual errors or logical fallacies detected"
            }
        
        # Rule 4: Too many high severity issues
        if high_count > self.convergence_thresholds["high_limit"] or high_count > 2:
            return {
                "action": "iterate", 
                "reasoning": f"Too many high severity issues ({high_count}) require revision",
                "convergence_score": 0.4,
                "confidence": "high",
                "detailed_reasoning": "Significant logical gaps or unsupported claims need addressing"
            }
        
        # Rule 5: Moderate issue threshold
        if total_issues > 5 and severity_score > 2.0:
            return {
                "action": "iterate",
                "reasoning": f"Overall issue severity ({severity_score:.2f}) exceeds acceptable threshold",
                "convergence_score": 0.5,
                "confidence": "medium", 
                "detailed_reasoning": f"Multiple issues with average severity {severity_score:.2f} require attention"
            }
        
        # Default: Converge with minor issues
        return {
            "action": "converged",
            "reasoning": f"Issues are minor and acceptable ({total_issues} total, severity {severity_score:.2f})",
            "convergence_score": 0.8,
            "confidence": "medium",
            "detailed_reasoning": "Quality meets acceptance criteria despite minor remaining issues"
        }
    
    def _check_convergence_conditions(self, severity_analysis: Dict[str, Any], current_round: int) -> bool:
        """Check if debate has converged based on NEW STRICT criteria"""
        
        critical_count = severity_analysis["by_severity"]["critical"]
        high_count = severity_analysis["by_severity"]["high"]
        medium_count = severity_analysis["by_severity"]["medium"]
        low_count = severity_analysis["by_severity"]["low"]
        total_issues = severity_analysis["total_issues"]
        
        # Rule 1: Perfect convergence - ZERO issues allowed
        if total_issues == 0:
            return True
        
        # Rule 2: NO convergence allowed if ANY issues exist UNLESS we've done 3+ iterations
        if current_round < 3:
            return False  # Always iterate if we haven't done 3 rounds yet
            
        # Rule 3: After 3+ iterations, only converge if NO CRITICAL/HIGH/MEDIUM issues remain
        if critical_count > 0 or high_count > 0 or medium_count > 0:
            return False  # Still have serious issues after 3 iterations - this will trigger escalation
        
        # Rule 4: After 3+ iterations with only LOW severity issues - allow convergence
        return True
    
    def _generate_convergence_reasoning(self, severity_analysis: Dict[str, Any]) -> str:
        """Generate human-readable convergence reasoning"""
        
        total_issues = severity_analysis["total_issues"]
        
        if total_issues == 0:
            return "No issues found - draft meets all quality criteria"
        
        issue_summary = []
        for severity, count in severity_analysis["by_severity"].items():
            if count > 0:
                issue_summary.append(f"{count} {severity}")
        
        return f"Draft acceptable with only minor issues: {', '.join(issue_summary)}"
    
    async def _generate_decision_content(
        self, 
        decision: Dict[str, Any], 
        critiques: List[Dict[str, Any]], 
        severity_analysis: Dict[str, Any], 
        current_round: int
    ) -> Dict[str, Any]:
        """Generate appropriate content based on decision"""
        
        content = {}
        
        if decision["action"] == "converged":
            # Prepare final package
            content["final_draft"] = {
                "status": "approved",
                "remaining_issues": [c for c in critiques if c.get('severity') == 'low'],
                "quality_score": decision["convergence_score"]
            }
            content["remaining_critiques"] = [c for c in critiques if c.get('severity') == 'low']
            
        elif decision["action"] == "iterate":
            # Generate focused feedback for revision
            content["feedback"] = await self._generate_revision_feedback(critiques, severity_analysis)
            
        elif decision["action"] == "escalate_with_warning":
            # Handle escalation - persistent issues after 3 iterations
            content["final_draft"] = {
                "status": "escalated",
                "quality_warning": True,
                "persistent_issues": decision["unresolved_issues"],
                "quality_score": decision["convergence_score"]
            }
            content["warning_message"] = self._generate_escalation_warning(critiques, severity_analysis)
            content["remaining_critiques"] = critiques
            
        elif decision["action"] == "abort_deadlock":
            # Handle deadlock situation
            content["final_draft"] = {
                "status": "deadlock",
                "partial_quality": True,
                "unresolved_issues": critiques,
                "quality_score": decision["convergence_score"]
            }
            content["remaining_critiques"] = critiques
        
        return content
    
    async def _generate_revision_feedback(
        self, 
        critiques: List[Dict[str, Any]], 
        severity_analysis: Dict[str, Any]
    ) -> str:
        """Generate actionable feedback for the Strategist to revise the draft"""
        
        if not critiques:
            return "No specific issues to address."
        
        # Priority order: critical > high > medium > low
        priority_critiques = sorted(
            critiques, 
            key=lambda x: {"critical": 4, "high": 3, "medium": 2, "low": 1}.get(x.get('severity', 'medium'), 2),
            reverse=True
        )
        
        feedback_sections = []
        
        # Group critiques by severity for structured feedback
        critical_issues = [c for c in critiques if c.get('severity') == 'critical']
        high_issues = [c for c in critiques if c.get('severity') == 'high']
        medium_issues = [c for c in critiques if c.get('severity') == 'medium']
        
        if critical_issues:
            feedback_sections.append("CRITICAL ISSUES (must fix):")
            for issue in critical_issues[:3]:  # Limit to avoid overwhelming feedback
                description = issue.get('description', 'No description')
                step_ref = issue.get('step_ref')
                step_info = f" (Step {step_ref})" if step_ref else ""
                feedback_sections.append(f"• {description}{step_info}")
        
        if high_issues:
            feedback_sections.append("\nHIGH PRIORITY ISSUES:")
            for issue in high_issues[:3]:
                description = issue.get('description', 'No description')
                step_ref = issue.get('step_ref') 
                step_info = f" (Step {step_ref})" if step_ref else ""
                feedback_sections.append(f"• {description}{step_info}")
        
        if medium_issues and len(critical_issues + high_issues) < 3:
            feedback_sections.append("\nMODERATE ISSUES:")
            for issue in medium_issues[:2]:
                description = issue.get('description', 'No description')
                feedback_sections.append(f"• {description}")
        
        # Add guidance
        feedback_sections.append("\nRevision Guidance:")
        
        if critical_issues or high_issues:
            feedback_sections.append("• Focus on addressing critical and high priority issues first")
        
        if severity_analysis["by_type"]["logic_flaw"] > 0:
            feedback_sections.append("• Review and strengthen logical reasoning connections")
        
        if severity_analysis["by_type"]["fact_contradiction"] > 0:
            feedback_sections.append("• Verify factual claims against provided context")
        
        if severity_analysis["by_type"]["hallucination"] > 0:
            feedback_sections.append("• Remove unsupported information not found in context")
        
        return "\n".join(feedback_sections)
    
    def _generate_escalation_warning(
        self, 
        critiques: List[Dict[str, Any]], 
        severity_analysis: Dict[str, Any]
    ) -> str:
        """Generate warning message for escalated quality issues"""
        
        critical_count = severity_analysis["by_severity"]["critical"]
        high_count = severity_analysis["by_severity"]["high"]
        medium_count = severity_analysis["by_severity"]["medium"]
        
        warning_parts = []
        warning_parts.append("️ QUALITY ESCALATION NOTICE:")
        warning_parts.append(f"After 3 improvement iterations, {critical_count + high_count + medium_count} serious issues remain unresolved:")
        
        if critical_count > 0:
            warning_parts.append(f"•{critical_count} CRITICAL issues (major factual errors, severe logical flaws)")
            
        if high_count > 0:
            warning_parts.append(f"•{high_count} HIGH severity issues (significant logical gaps, unsupported claims)")
            
        if medium_count > 0:
            warning_parts.append(f"•{medium_count} MEDIUM severity issues (inconsistencies, missing details)")
            
        warning_parts.append("\nThe system has made its best effort to improve this response, but some quality concerns persist.")
        warning_parts.append("Please review the answer critically and consider these limitations when using the information.")
        
        # Add specific issue details
        if len(critiques) > 0:
            warning_parts.append("\nSpecific concerns:")
            for issue in critiques[:3]:  # Show top 3 issues
                severity = issue.get('severity', 'unknown').upper()
                description = issue.get('description', 'No description')[:100]
                warning_parts.append(f"• [{severity}] {description}...")
                
        return "\n".join(warning_parts)
    
    def get_decision_statistics(self) -> Dict[str, Any]:
        """Get statistics about moderator decisions"""
        return {
            "convergence_rate": getattr(self, 'convergence_count', 0) / max(self.execution_count, 1),
            "average_rounds": getattr(self, 'total_rounds', 0) / max(self.execution_count, 1),
            "deadlock_rate": getattr(self, 'deadlock_count', 0) / max(self.execution_count, 1)
        } 

🔹 FILE: ./agents/base_agent.py
-----------------------------------
"""
Base Agent Class for Speculative AI Multi-Agent System

Simplified base class without over-engineered registry system.
"""

import time
import logging
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, List
from dataclasses import dataclass
from enum import Enum

from ai_agents.config import SpeculativeAIConfig


class AgentRole(Enum):
    """Agent roles in the speculative AI system"""
    RETRIEVE = "retrieve" 
    STRATEGIST = "strategist"
    CRITIC = "critic"
    MODERATOR = "moderator"
    REPORTER = "reporter"
    TUTOR = "tutor"


@dataclass
class AgentInput:
    """Standardized input format for all agents"""
    query: str
    context: List[Dict[str, Any]]
    metadata: Dict[str, Any]
    session_id: str


@dataclass 
class AgentOutput:
    """Standardized output format for all agents"""
    success: bool
    content: Dict[str, Any]
    metadata: Dict[str, Any]
    processing_time: float
    agent_role: AgentRole
    error_message: Optional[str] = None


class BaseAgent(ABC):
    """Simplified base class for all agents"""
    
    def __init__(
        self,
        agent_role: AgentRole,
        config: SpeculativeAIConfig,
        llm_client=None,
        logger: Optional[logging.Logger] = None
    ):
        self.agent_role = agent_role
        self.config = config
        self.llm_client = llm_client
        self.logger = logger or logging.getLogger(f"ai_agents.{agent_role.value}")
        
        # Simple performance tracking
        self.execution_count = 0
        self.total_processing_time = 0.0
        self.error_count = 0
    
    def get_temperature(self) -> float:
        """Get appropriate temperature setting for this agent"""
        if self.agent_role == AgentRole.STRATEGIST:
            return self.config.strategist_temperature
        elif self.agent_role == AgentRole.CRITIC:
            return self.config.critic_temperature
        else:
            return 0.3  # Default conservative temperature
    
    async def execute(self, agent_input: AgentInput) -> AgentOutput:
        """Execute the agent with error handling and metrics tracking"""
        start_time = time.time()
        self.execution_count += 1
        
        try:
            result = await self.process(agent_input)
            processing_time = time.time() - start_time
            self.total_processing_time += processing_time
            
            if result.success and self.config.enable_debug_logging:
                self.logger.info(f"{self.agent_role.value.title()} completed in {processing_time:.3f}s")
            
            return result
            
        except Exception as e:
            self.error_count += 1
            processing_time = time.time() - start_time
            error_msg = f"{self.agent_role.value} error: {str(e)}"
            
            self.logger.error(f"{error_msg}")
            
            return AgentOutput(
                success=False,
                content={},
                metadata={"error_type": type(e).__name__},
                processing_time=processing_time,
                agent_role=self.agent_role,
                error_message=error_msg
            )
    
    @abstractmethod
    async def process(self, agent_input: AgentInput) -> AgentOutput:
        """Process the agent input and return output"""
        pass
    
    def get_metrics(self) -> Dict[str, Any]:
        """Get simple performance metrics"""
        avg_time = self.total_processing_time / max(self.execution_count, 1)
        return {
            "executions": self.execution_count,
            "total_time": self.total_processing_time,
            "average_time": avg_time,
            "error_count": self.error_count,
            "success_rate": (self.execution_count - self.error_count) / max(self.execution_count, 1)
        } 

🔹 FILE: ./agents/retrieve_agent.py
-----------------------------------
"""
Retrieve Agent - Enhanced RAG Integration

This agent integrates directly with the existing RAG system and adds
speculative query reframing capabilities on top of it.
"""

import asyncio
from typing import Dict, Any, List, Optional

from ai_agents.agents.base_agent import BaseAgent, AgentRole, AgentInput, AgentOutput
from ai_agents.config import SpeculativeAIConfig


class RetrieveAgent(BaseAgent):
    """Enhanced retrieval agent that integrates with existing RAG system"""
    
    def __init__(
        self,
        config: SpeculativeAIConfig,
        llm_client=None,
        rag_service=None,
        logger=None
    ):
        super().__init__(
            agent_role=AgentRole.RETRIEVE,
            config=config,
            llm_client=llm_client,
            logger=logger
        )
        
        self.rag_service = rag_service
        
        # Quality thresholds for triggering speculative retrieval
        self.min_quality_threshold = 0.7
        self.min_results_count = 3
        
    async def process(self, agent_input: AgentInput) -> AgentOutput:
        """Execute enhanced retrieval with speculative query reframing"""
        
        # Extract course_id from metadata  
        course_id = agent_input.metadata.get('course_id')
        if not course_id:
            return AgentOutput(
                success=False,
                content={},
                metadata={},
                processing_time=0.0,
                agent_role=self.agent_role,
                error_message="course_id required for retrieval"
            )
        
        # Stage 1: Initial RAG retrieval using existing system
        self.logger.info("\n" + "="*250)
        self.logger.info("INITIAL RAG RETRIEVAL")
        self.logger.info("="*250)
        self.logger.info(f"Query: '{agent_input.query}'")
        self.logger.info(f"Course ID: {course_id}")
        self.logger.info("Performing initial retrieval...\n")
        
        initial_results = await self._perform_rag_query(agent_input.query, course_id)
        
        if not initial_results:
            self.logger.info("Initial RAG retrieval completely failed")
            return AgentOutput(
                success=False,
                content={"retrieval_results": [], "quality_assessment": {"score": 0.0}},
                metadata={"retrieval_strategy": "failed"},
                processing_time=0.0,
                agent_role=self.agent_role,
                error_message="Initial RAG retrieval failed"
            )
        
        # Log initial results with clear formatting
        sources = initial_results.get('sources', [])
        self.logger.info(f"Initial RAG completed - found {len(sources)} sources:")
        for i, source in enumerate(sources[:3]):
            score = source.get('score', 'N/A')
            content = source.get('content', '')
            self.logger.info(f"  {i+1}. Score={score}, Content='{content}'")
        if len(sources) > 3:
            self.logger.info(f"  ... and {len(sources) - 3} more sources")
        
        # Stage 2: Quality assessment 
        self.logger.info("\n" + "="*250)
        self.logger.info("RETRIEVAL QUALITY ASSESSMENT")
        self.logger.info("="*250)
        
        quality_score = self._assess_retrieval_quality(initial_results)
        self.logger.info(f"Quality Score: {quality_score:.3f} / 1.0")
        self.logger.info(f"Quality Threshold: {self.min_quality_threshold}")
        self.logger.info(f"Initial Results Count: {len(initial_results.get('sources', []))}")
        
        # Stage 3: Speculative reframing if quality is poor
        if quality_score < self.min_quality_threshold and self.llm_client:
            self.logger.info(f"\nQUALITY TOO LOW - TRIGGERING SPECULATIVE REFRAMING")
            self.logger.info(f"Original query not good enough (score {quality_score:.3f} < {self.min_quality_threshold})")
            self.logger.info(f"Generating alternative query phrasings...\n")
            
            reframed_results = await self._speculative_reframing(
                agent_input.query, 
                course_id, 
                initial_results
            )
            
            if reframed_results:
                self.logger.info("\n" + "="*250)
                self.logger.info("MERGING SPECULATIVE RESULTS")
                self.logger.info("="*250)
                # Merge and deduplicate results
                final_results = self._merge_results(initial_results, reframed_results)
                strategy = "speculative_enhanced"
                new_quality = self._assess_retrieval_quality(final_results)
                self.logger.info(f"Enhanced quality: {quality_score:.3f} → {new_quality:.3f}")
                self.logger.info(f"Total sources after merging: {len(final_results.get('sources', []))}")
            else:
                self.logger.info("\nSpeculative reframing failed - using initial results only")
                final_results = initial_results
                strategy = "initial_only"
        else:
            if quality_score >= self.min_quality_threshold:
                self.logger.info(f"\nQUALITY SUFFICIENT - NO REFRAMING NEEDED")
                self.logger.info(f"Score {quality_score:.3f} meets threshold {self.min_quality_threshold}")
            else:
                self.logger.info(f"\nQuality low but no LLM client available for reframing")
            final_results = initial_results
            strategy = "initial_sufficient"
        
        # Format final results for downstream agents
        self.logger.info("\n" + "="*250)
        self.logger.info("FINAL RETRIEVAL SUMMARY")
        self.logger.info("="*250)
        
        formatted_context = self._format_for_agents(final_results)
        
        self.logger.info(f"Strategy Used: {strategy}")
        self.logger.info(f"Raw Sources: {len(final_results.get('sources', []))}")
        self.logger.info(f"Formatted Items: {len(formatted_context)}")
        
        # Log clean chunk summary with better formatting
        self._log_retrieved_chunks(formatted_context, strategy)
        self.logger.info("="*250)
        
        return AgentOutput(
            success=True,
            content={
                "retrieval_results": formatted_context,
                "quality_assessment": {
                    "score": max(quality_score, self._assess_retrieval_quality(final_results)),
                    "initial_count": len(initial_results.get('sources', [])),
                    "final_count": len(formatted_context)
                }
            },
            metadata={
                "retrieval_strategy": strategy,
                "quality_improvement": strategy == "speculative_enhanced"
            },
            processing_time=0.0,
            agent_role=self.agent_role
        )
    
    async def _perform_rag_query(self, query: str, course_id: str) -> Optional[Dict[str, Any]]:
        """Perform RAG query using existing system"""
        try:
            # DEBUG: Always show first 3 chunks from this course for sanity check
            await self._debug_course_chunks(course_id, query)
            
            if self.rag_service:
                return self.rag_service.answer_question(course_id, query)
            else:
                self.logger.warning("No RAG service available - using mock results")
                return {
                    "success": True,
                    "answer": "Mock RAG response",
                    "sources": [{"content": "Mock content", "score": 0.8}]
                }
        except Exception as e:
            self.logger.error(f"RAG query failed: {e}")
            return None
    
    def _assess_retrieval_quality(self, rag_result: Dict[str, Any]) -> float:
        """Assess quality of RAG retrieval results"""
        if not rag_result or not rag_result.get('success'):
            return 0.0
        
        sources = rag_result.get('sources', [])
        if len(sources) < self.min_results_count:
            return 0.3
        
        # Calculate average relevance score
        scores = []
        for source in sources:
            score = source.get('score', 0)
            if score and score != 'N/A':
                try:
                    scores.append(float(score))
                except (ValueError, TypeError):
                    continue
        if not scores:
            return 0.5
        
        avg_score = sum(scores) / len(scores)
        return min(avg_score * 1.2, 1.0)  # Boost slightly, cap at 1.0
    
    async def _speculative_reframing(
        self, 
        original_query: str, 
        course_id: str, 
        initial_results: Dict[str, Any]
    ) -> Optional[Dict[str, Any]]:
        """Generate alternative queries and perform parallel retrieval"""
        
        try:
            self.logger.info("\n" + "="*250)
            self.logger.info("SPECULATIVE QUERY GENERATION")
            self.logger.info("="*250)
            self.logger.info(f"Original Query: '{original_query}'")
            self.logger.info(f"Course ID: {course_id}")
            self.logger.info("Generating alternative query phrasings...\n")
            
            # Generate alternative queries using LLM
            alternative_queries = await self._generate_alternative_queries(original_query)
            
            if not alternative_queries:
                self.logger.info("No alternative queries generated")
                return None
            
            self.logger.info(f"Generated {len(alternative_queries)} alternative queries:")
            for i, query in enumerate(alternative_queries, 1):
                self.logger.info(f"  {i}. '{query}'")
            
            self.logger.info("\n" + "-"*250)
            self.logger.info("PARALLEL RETRIEVAL FOR ALTERNATIVES")
            self.logger.info("-"*250)
            
            # Perform parallel retrieval for alternative queries
            tasks = []
            for alt_query in alternative_queries:
                task = self._perform_rag_query(alt_query, course_id)
                tasks.append(task)
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Find the best alternative result
            self.logger.info("\nEvaluating alternative query results:")
            best_result = None
            best_score = 0.0
            best_query_idx = -1
            
            for i, (result, query) in enumerate(zip(results, alternative_queries)):
                if isinstance(result, dict) and result.get('success'):
                    score = self._assess_retrieval_quality(result)
                    sources_count = len(result.get('sources', []))
                    self.logger.info(f"  Alternative {i+1}: Score={score:.3f}, Sources={sources_count}")
                    if score > best_score:
                        best_score = score
                        best_result = result
                        best_query_idx = i
                else:
                    self.logger.info(f"  Alternative {i+1}: FAILED")
            
            if best_result:
                self.logger.info(f"\nBEST ALTERNATIVE FOUND:")
                self.logger.info(f"  Query: '{alternative_queries[best_query_idx]}'")
                self.logger.info(f"  Score: {best_score:.3f}")
                self.logger.info(f"  Sources: {len(best_result.get('sources', []))}")
                return best_result
            else:
                self.logger.info(f"\nNo successful alternative queries")
                return None
            
        except Exception as e:
            self.logger.error(f"Speculative reframing failed: {e}")
            return None
    
    async def _generate_alternative_queries(self, original_query: str) -> List[str]:
        """Generate alternative queries using LLM"""
        try:
            prompt = f"""
            The original query "{original_query}" didn't retrieve high-quality results.
            Generate 2-3 alternative queries that might find better information:
            
            1. A more specific version
            2. A broader conceptual version  
            3. A query using different terminology
            
            Return only the alternative queries, one per line.
            """
            
            self.logger.info("\n" + "-"*250)
            self.logger.info("LLM QUERY GENERATION")
            self.logger.info("-"*250)
            self.logger.info("PROMPT:")
            self.logger.info(prompt.strip())
            self.logger.info("-"*250)
            
            response = self.llm_client.generate(prompt)
            
            self.logger.info("LLM RESPONSE:")
            self.logger.info(response.strip())
            self.logger.info("-"*250)
            
            queries = [q.strip() for q in response.split('\n') if q.strip() and not q.startswith('#')]
            parsed_queries = queries[:3]  # Limit to 3 alternatives
            
            self.logger.info(f"PARSED QUERIES ({len(parsed_queries)}):")
            for i, q in enumerate(parsed_queries, 1):
                self.logger.info(f"  {i}. {q}")
            
            return parsed_queries
            
        except Exception as e:
            self.logger.error(f"Alternative query generation failed: {e}")
            return []
    
    def _merge_results(
        self, 
        initial_results: Dict[str, Any], 
        reframed_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Merge and deduplicate results from multiple retrievals"""
        
        initial_sources = initial_results.get('sources', [])
        reframed_sources = reframed_results.get('sources', [])
        
        # Simple deduplication by content similarity
        merged_sources = initial_sources.copy()
        
        for new_source in reframed_sources:
            new_content = new_source.get('content', '')
            is_duplicate = False
            
            for existing_source in merged_sources:
                existing_content = existing_source.get('content', '')
                # Simple overlap check
                if len(set(new_content.split()) & set(existing_content.split())) > len(new_content.split()) * 0.7:
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                merged_sources.append(new_source)
        
        # Return merged result
        return {
            "success": True,
            "answer": reframed_results.get('answer', initial_results.get('answer')),
            "sources": merged_sources
        }
    
    def _format_for_agents(self, rag_result: Dict[str, Any]) -> List[Dict[str, Any]]:
        """Format RAG results for downstream agent consumption"""
        
        if not rag_result or not rag_result.get('success'):
            return []
        
        sources = rag_result.get('sources', [])
        formatted_results = []
        
        for i, source in enumerate(sources):
            formatted_item = {
                "index": i,
                "content": source.get('content', ''),
                "score": source.get('score', 0.0),
                "source": source.get('metadata', {})
            }
            formatted_results.append(formatted_item)
        
        return formatted_results
    
    def _log_retrieved_chunks(self, chunks: List[Dict[str, Any]], strategy: str):
        """Log retrieved chunks in a clean, organized format"""
        if not chunks:
            self.logger.info("No chunks retrieved")
            return
            
        self.logger.info(f"Retrieved {len(chunks)} chunks using {strategy}:")
        for chunk in chunks:
            score = chunk.get('score', 0.0)
            # Handle the score properly - it might be 'N/A', None, or a number
            if score == 'N/A' or score is None:
                score_display = "N/A"
            else:
                try:
                    score_float = float(score)
                    score_display = f"{score_float:.3f}"
                except (ValueError, TypeError):
                    score_display = "N/A"
            
            content_preview = chunk.get('content', '')
            self.logger.info(f"   • Similarity: {score_display} | {content_preview}")
    
    async def _debug_course_chunks(self, course_id: str, actual_query: str = None):
        """Debug: Show first 3 chunks from this course with similarity scores"""
        try:
            if not self.rag_service:
                self.logger.info(f"DEBUG: No RAG service available for course {course_id}")
                return
                
            # Get the vector client directly to query raw chunks
            vector_client = getattr(self.rag_service, 'vector_client', None)
            if not vector_client:
                self.logger.info(f"DEBUG: No vector client available for course {course_id}")
                return
                
            # Query for any 3 documents from this course (no similarity filtering)
            try:
                # First, get any chunks to show they exist
                raw_results = vector_client.similarity_search(
                    query="course content", 
                    k=3, 
                    filter={"course_id": course_id}
                )
                
                if raw_results:
                    self.logger.info(f"DEBUG: Found {len(raw_results)} chunks in course {course_id}")
                    
                    # If we have the actual query, show similarity scores with that query
                    if actual_query:
                        try:
                            scored_results = vector_client.similarity_search_with_score(
                                query=actual_query,
                                k=3,
                                filter={"course_id": course_id}
                            )
                            
                            self.logger.info(f"DEBUG: Similarity scores for query '{actual_query[:50]}...':")
                            for i, (doc, score) in enumerate(scored_results, 1):
                                content_preview = doc.page_content
                                metadata = doc.metadata or {}
                                chunk_id = metadata.get('chunk_index', 'unknown')
                                self.logger.info(f"   {i}. Chunk {chunk_id} | Score: {score:.4f} | {content_preview}")
                                
                        except Exception as score_error:
                            self.logger.error(f"DEBUG: Error getting similarity scores: {score_error}")
                            # Fallback to showing chunks without scores
                            for i, doc in enumerate(raw_results, 1):
                                content_preview = doc.page_content
                                metadata = doc.metadata or {}
                                chunk_id = metadata.get('chunk_index', 'unknown')
                                self.logger.info(f"   {i}. Chunk {chunk_id}: {content_preview}")
                    else:
                        # Just show the chunks without scores
                        for i, doc in enumerate(raw_results, 1):
                            content_preview = doc.page_content[:80] + '...' if len(doc.page_content) > 80 else doc.page_content
                            metadata = doc.metadata or {}
                            chunk_id = metadata.get('chunk_index', 'unknown')
                            self.logger.info(f"   {i}. Chunk {chunk_id}: {content_preview}")
                else:
                    self.logger.info(f"DEBUG: No chunks found in course {course_id} database")
                    
            except Exception as search_error:
                self.logger.error(f"DEBUG: Error searching course {course_id}: {search_error}")
                
        except Exception as e:
            self.logger.error(f"DEBUG: Failed to debug course chunks: {e}")

    def get_agent_metrics(self) -> Dict[str, Any]:
        """Get retrieval-specific metrics"""
        base_metrics = self.get_metrics()
        return {
            **base_metrics,
            "retrieval_strategy_distribution": getattr(self, '_strategy_stats', {}),
            "average_quality_score": getattr(self, '_avg_quality', 0.0)
        } 

🔹 FILE: ./agents/__init__.py
-----------------------------------
# Empty __init__.py to maintain package structure 

🔹 FILE: ./agents/tutor_agent.py
-----------------------------------
"""
Tutor Agent - Intelligent User Interaction Manager

Manages direct interaction with users, transforming Q&A into dynamic, personalized learning experiences.
"""

import time
import logging
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta

from ai_agents.agents.base_agent import BaseAgent, AgentInput, AgentOutput, AgentRole


class TutorAgent(BaseAgent):
    """
    Tutor Agent - Manages user interaction lifecycle
    
    Responsibilities:
    - Guide: Present contextual guidance before answers
    - Analyze: Monitor user behavior patterns
    - Test: Generate comprehension assessments
    - Discipline: Enforce learning-focused interactions
    """
    
    def __init__(self, config, llm_client=None, logger=None):
        super().__init__(agent_role=AgentRole.TUTOR, config=config, llm_client=llm_client, logger=logger)
        
        # User session tracking
        self.user_sessions = {}
        
        # Behavioral analysis thresholds
        self.similarity_threshold = 0.8
        self.consecutive_similar_limit = 3
        self.comprehension_threshold = 0.6
        self.cooldown_duration_minutes = 15
        
    async def process(self, agent_input: AgentInput) -> AgentOutput:
        """Process tutor interaction request"""
        try:
            session_id = agent_input.session_id
            query = agent_input.query
            metadata = agent_input.metadata
            
            # Extract final answer from Reporter
            final_answer = metadata.get("final_answer", {})
            conversation_history = metadata.get("conversation_history", [])
            
            # Initialize or update user session
            self._update_user_session(session_id, query, conversation_history)
            
            # Determine current interaction state
            interaction_state = self._analyze_user_behavior(session_id, query)
            
            # Generate appropriate response based on state
            if interaction_state == "cooldown":
                response = self._generate_cooldown_response(session_id)
            elif interaction_state == "test":
                response = self._generate_test_interaction(session_id, final_answer)
            else:  # guide or normal
                response = self._generate_guided_response(session_id, final_answer, query)
            
            return AgentOutput(
                success=True,
                content=response,
                metadata={
                    "interaction_state": interaction_state,
                    "session_metrics": self._get_session_metrics(session_id)
                },
                processing_time=time.time(),
                agent_role=self.agent_role
            )
            
        except Exception as e:
            self.logger.error(f"Tutor processing failed: {e}")
            return AgentOutput(
                success=False,
                content={},
                metadata={},
                processing_time=time.time(),
                agent_role=self.agent_role,
                error_message=str(e)
            )
    
    def _update_user_session(self, session_id: str, query: str, history: List[Dict]):
        """Update user session tracking"""
        if session_id not in self.user_sessions:
            self.user_sessions[session_id] = {
                "queries": [],
                "state": "guide",
                "test_scores": [],
                "cooldown_until": None,
                "created_at": datetime.now()
            }
        
        session = self.user_sessions[session_id]
        session["queries"].append({
            "query": query,
            "timestamp": datetime.now(),
            "vector": self._vectorize_query(query)  # Simplified - would use actual embedding
        })
        
        # Keep only recent queries (last 10)
        if len(session["queries"]) > 10:
            session["queries"] = session["queries"][-10:]
    
    def _analyze_user_behavior(self, session_id: str, current_query: str) -> str:
        """Analyze user behavior to determine interaction state"""
        session = self.user_sessions.get(session_id, {})
        
        # Check cooldown status
        cooldown_until = session.get("cooldown_until")
        if cooldown_until and datetime.now() < cooldown_until:
            return "cooldown"
        
        # Analyze query similarity patterns
        queries = session.get("queries", [])
        if len(queries) >= self.consecutive_similar_limit:
            recent_queries = queries[-self.consecutive_similar_limit:]
            similarity_count = self._count_similar_queries(recent_queries)
            
            if similarity_count >= self.consecutive_similar_limit - 1:
                # Switch to test mode
                session["state"] = "test"
                return "test"
        
        return session.get("state", "guide")
    
    def _count_similar_queries(self, queries: List[Dict]) -> int:
        """Count similar queries in the list"""
        # Simplified similarity check - in production would use actual vector similarity
        if len(queries) < 2:
            return 0
        
        similar_count = 0
        for i in range(1, len(queries)):
            if self._calculate_similarity(queries[i]["query"], queries[i-1]["query"]) > self.similarity_threshold:
                similar_count += 1
        
        return similar_count
    
    def _calculate_similarity(self, query1: str, query2: str) -> float:
        """Calculate query similarity (simplified implementation)"""
        # Simplified Jaccard similarity - in production would use vector similarity
        words1 = set(query1.lower().split())
        words2 = set(query2.lower().split())
        
        if not words1 and not words2:
            return 1.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union) if union else 0.0
    
    def _vectorize_query(self, query: str) -> List[float]:
        """Vectorize query for similarity analysis (simplified)"""
        # Simplified - in production would use actual embedding model
        return [hash(word) % 100 / 100.0 for word in query.split()[:10]]
    
    def _generate_guided_response(self, session_id: str, final_answer: Dict, query: str) -> Dict:
        """Generate guided learning response"""
        return {
            "interaction_type": "guided_learning",
            "elements": [
                {
                    "type": "guidance",
                    "content": self._generate_guidance_text(query, final_answer)
                },
                {
                    "type": "answer",
                    "content": final_answer
                },
                {
                    "type": "reflection_prompt",
                    "content": self._generate_reflection_prompt(final_answer)
                }
            ]
        }
    
    def _generate_test_interaction(self, session_id: str, final_answer: Dict) -> Dict:
        """Generate comprehension test"""
        quiz_questions = self._generate_quiz_questions(final_answer)
        
        return {
            "interaction_type": "comprehension_test",
            "elements": [
                {
                    "type": "test_intro",
                    "content": "I notice you've asked similar questions recently. Let's check your understanding with a quick quiz:"
                },
                {
                    "type": "answer",
                    "content": final_answer
                },
                {
                    "type": "quiz",
                    "content": quiz_questions
                }
            ]
        }
    
    def _generate_cooldown_response(self, session_id: str) -> Dict:
        """Generate cooldown message"""
        session = self.user_sessions[session_id]
        cooldown_until = session.get("cooldown_until")
        remaining_time = cooldown_until - datetime.now() if cooldown_until else timedelta(0)
        
        return {
            "interaction_type": "cooldown",
            "elements": [
                {
                    "type": "cooldown_message",
                    "content": f"It seems you need more time to review the material. Please take {remaining_time.seconds // 60} more minutes to study the provided resources before asking new questions."
                },
                {
                    "type": "study_resources",
                    "content": self._get_study_recommendations(session_id)
                }
            ]
        }
    
    def _generate_guidance_text(self, query: str, final_answer: Dict) -> str:
        """Generate contextual guidance text"""
        # Simplified guidance generation
        topic_keywords = self._extract_topic_keywords(query)
        
        if any(keyword in query.lower() for keyword in ["lagrange", "optimization", "constraint"]):
            return "Before diving into this optimization problem, recall that Lagrange multipliers help us find extrema of functions subject to constraints. The key insight is that at the optimal point, the gradients must be parallel."
        elif any(keyword in query.lower() for keyword in ["circuit", "resistance", "voltage"]):
            return "Let's approach this circuit analysis step by step. Remember Ohm's law and Kirchhoff's rules as our fundamental tools."
        else:
            return "Let's work through this problem systematically, building on the fundamental concepts."
    
    def _generate_reflection_prompt(self, final_answer: Dict) -> str:
        """Generate reflection prompt based on answer content"""
        return "Take a moment to consider: What was the key insight that made this solution work? How might you apply this approach to similar problems?"
    
    def _generate_quiz_questions(self, final_answer: Dict) -> Dict:
        """Generate quiz questions based on final answer"""
        # Simplified quiz generation - in production would use LLM
        return {
            "questions": [
                {
                    "id": 1,
                    "question": "What is the main concept demonstrated in this solution?",
                    "type": "multiple_choice",
                    "options": ["A) Basic algebra", "B) Advanced calculus", "C) The core principle from the answer", "D) Memorization"],
                    "correct": "C"
                }
            ]
        }
    
    def _extract_topic_keywords(self, query: str) -> List[str]:
        """Extract topic keywords from query"""
        # Simplified keyword extraction
        return [word.lower() for word in query.split() if len(word) > 3]
    
    def _get_study_recommendations(self, session_id: str) -> List[str]:
        """Get personalized study recommendations"""
        return [
            "Review the fundamental concepts covered in the recent answers",
            "Practice similar problems with different parameters",
            "Focus on understanding the underlying principles rather than memorizing steps"
        ]
    
    def _get_session_metrics(self, session_id: str) -> Dict:
        """Get session performance metrics"""
        session = self.user_sessions.get(session_id, {})
        return {
            "total_queries": len(session.get("queries", [])),
            "current_state": session.get("state", "guide"),
            "average_test_score": sum(session.get("test_scores", [])) / max(len(session.get("test_scores", [])), 1),
            "session_duration": (datetime.now() - session.get("created_at", datetime.now())).total_seconds()
        }
    
    def process_quiz_response(self, session_id: str, quiz_response: Dict) -> Dict:
        """Process user's quiz response and update state"""
        session = self.user_sessions.get(session_id, {})
        
        # Calculate score (simplified)
        score = self._calculate_quiz_score(quiz_response)
        session.setdefault("test_scores", []).append(score)
        
        if score < self.comprehension_threshold:
            # Trigger cooldown
            session["cooldown_until"] = datetime.now() + timedelta(minutes=self.cooldown_duration_minutes)
            session["state"] = "cooldown"
            
            return {
                "result": "needs_review",
                "score": score,
                "message": "Your score indicates you need more time to review the material."
            }
        else:
            # Reset to normal state
            session["state"] = "guide"
            return {
                "result": "passed",
                "score": score,
                "message": "Great job! You can continue asking questions."
            }
    
    def _calculate_quiz_score(self, quiz_response: Dict) -> float:
        """Calculate quiz score"""
        # Simplified scoring - in production would be more sophisticated
        correct_answers = quiz_response.get("correct_count", 0)
        total_questions = quiz_response.get("total_questions", 1)
        return correct_answers / total_questions 

🔹 FILE: ./agents/critic_agent.py
-----------------------------------
"""
Critic Agent - Critical Verifier

This agent performs ruthless, evidence-based review of draft solutions,
identifying logical flaws, factual errors, and hallucinations.
"""

import re
from typing import List, Dict, Any, Tuple
from ai_agents.agents.base_agent import BaseAgent, AgentInput, AgentOutput, AgentRole


class CriticAgent(BaseAgent):
    """
    Critic Agent - Critical Verifier
    
    Responsible for:
    1. Logical verification of Chain-of-Thought steps
    2. Fact-checking against provided context
    3. Hallucination detection
    4. Generating structured critique reports
    """
    
    def __init__(self, config, llm_client=None, logger=None):
        super().__init__(AgentRole.CRITIC, config, llm_client, logger)
        
        # Critic-specific settings
        self.system_prompt = self._build_system_prompt()
        
        # Severity levels for critiques
        self.severity_levels = {
            "critical": 4,    # Major factual errors, logical fallacies
            "high": 3,        # Significant logical gaps, unsupported claims
            "medium": 2,      # Minor inconsistencies, missing details
            "low": 1         # Style issues, minor improvements
        }
        
    def _build_system_prompt(self) -> str:
        """Build the system prompt for the critic"""
        return """
        You are a rigorous academic critic and fact-checker. Your role is to:

        1. ANALYZE the draft solution and Chain-of-Thought for logical consistency
        2. VERIFY factual claims against the provided context
        3. DETECT any hallucinated or unsupported information
        4. IDENTIFY logical gaps, fallacies, or inconsistencies

        Critical principles:
        - Be ruthless but constructive in your analysis
        - Focus on evidence-based verification
        - Identify specific issues with precise references
        - Classify issues by severity: critical, high, medium, low
        - Do NOT provide corrections, only identify problems
        - Think methodically and document reasoning for each critique

        Your output should be a structured critique report suitable for revision guidance.
        """
    
    async def process(self, agent_input: AgentInput) -> AgentOutput:
        """
        Perform critical verification of draft solution
        
        Args:
            agent_input: Contains draft, CoT, context, and metadata
            
        Returns:
            AgentOutput: Structured critique report with issues identified
        """
        import time
        start_time = time.time()
        
        try:
            # Extract data from input
            draft_content = agent_input.metadata.get('draft_content', '')
            chain_of_thought = agent_input.metadata.get('chain_of_thought', [])
            context = agent_input.context
            
            self.logger.info("\n" + "="*250)
            self.logger.info("CRITIC AGENT - DRAFT ANALYSIS")
            self.logger.info("="*250)
            self.logger.info(f"CoT steps: {len(chain_of_thought)}")
            self.logger.info(f"Context items: {len(context)}")
            self.logger.info(f"Draft length: {len(draft_content)} characters")
            
            # ULTRA VERBOSE: Show the actual content being analyzed
            self.logger.info(f"\n--- DRAFT CONTENT TO ANALYZE ---")
            self.logger.info(f"'{draft_content}'")
            
            self.logger.info(f"\n--- CONTEXT ITEMS FOR VERIFICATION ---")
            for i, ctx_item in enumerate(context[:3]):
                content = ctx_item.get('content', ctx_item.get('text', str(ctx_item)))
                self.logger.info(f"Context {i+1}: '{content}'")
            if len(context) > 3:
                self.logger.info(f"... and {len(context) - 3} more context items")
                
            self.logger.info(f"\n--- CHAIN OF THOUGHT STEPS ---")
            for i, step in enumerate(chain_of_thought[:3]):
                step_text = step.get('thought', str(step))
                self.logger.info(f"CoT Step {i+1}: '{step_text}...'")
            if len(chain_of_thought) > 3:
                self.logger.info(f"... and {len(chain_of_thought) - 3} more CoT steps")
            
            # Perform three core verification tasks in parallel
            critiques = []
            
            # Task 1: Logical Verification
            self.logger.info("\n" + "-"*250)
            self.logger.info("PHASE 1: LOGICAL CONSISTENCY VERIFICATION")
            self.logger.info("-"*250)
            logic_start = time.time()
            logic_critiques = await self._verify_logical_consistency(chain_of_thought, draft_content)
            logic_time = time.time() - logic_start
            self.logger.info(f"Logical verification completed in {logic_time:.2f}s")
            
            # Task 2: Factual Verification
            self.logger.info("\n" + "-"*250)
            self.logger.info("PHASE 2: FACTUAL ACCURACY VERIFICATION")
            self.logger.info("-"*250)
            fact_start = time.time()
            factual_issues = await self._verify_factual_accuracy(draft_content, context)
            fact_time = time.time() - fact_start
            self.logger.info(f"Factual verification completed in {fact_time:.2f}s")
            
            # Task 3: Hallucination Detection
            self.logger.info("\n" + "-"*250)
            self.logger.info("PHASE 3: HALLUCINATION DETECTION")
            self.logger.info("-"*250)
            halluc_start = time.time()
            hallucination_issues = await self._detect_hallucinations(draft_content, context)
            halluc_time = time.time() - halluc_start
            self.logger.info(f"Hallucination detection completed in {halluc_time:.2f}s")
            
            # Combine all critiques
            self.logger.info("\n" + "-"*250)
            self.logger.info("COMBINING ALL CRITIQUES")
            self.logger.info("-"*250)
            critiques.extend(logic_critiques)
            critiques.extend(factual_issues)
            critiques.extend(hallucination_issues)
            
            # Assess overall critique severity
            overall_assessment = self._assess_overall_quality(critiques)
            
            # ULTRA VERBOSE: Log final assessment details
            self.logger.info("\n" + "="*250)
            self.logger.info("FINAL CRITIC ASSESSMENT")
            self.logger.info("="*250)
            self.logger.info(f"TOTAL ISSUES FOUND: {len(critiques)}")
            
            # Break down by severity
            critical_count = len([c for c in critiques if c.get('severity') == 'critical'])
            high_count = len([c for c in critiques if c.get('severity') == 'high'])
            medium_count = len([c for c in critiques if c.get('severity') == 'medium'])
            low_count = len([c for c in critiques if c.get('severity') == 'low'])
            
            self.logger.info(f"CRITICAL: {critical_count}")
            self.logger.info(f"HIGH: {high_count}")
            self.logger.info(f"MEDIUM: {medium_count}")
            self.logger.info(f"LOW: {low_count}")
            
            # Break down by type
            logic_count = len([c for c in critiques if c.get('type') == 'logic_flaw'])
            fact_count = len([c for c in critiques if c.get('type') == 'fact_contradiction'])
            halluc_count = len([c for c in critiques if c.get('type') == 'hallucination'])
            
            self.logger.info(f"LOGICAL ISSUES: {logic_count}")
            self.logger.info(f"FACTUAL ISSUES: {fact_count}")
            self.logger.info(f"HALLUCINATION ISSUES: {halluc_count}")
            
            self.logger.info(f"OVERALL ASSESSMENT: {overall_assessment.upper()}")
            
            if critiques:
                self.logger.info(f"ISSUE DETAILS:")
                for i, issue in enumerate(critiques, 1):
                    severity = issue.get('severity', 'unknown').upper()
                    issue_type = issue.get('type', 'unknown').upper()
                    desc = issue.get('description', 'no description')
                    self.logger.info(f"   {i}. [{severity}] {issue_type}: {desc}...")
            else:
                self.logger.info(f"NO ISSUES DETECTED - DRAFT IS CLEAN!")
            
            self.logger.info("\n" + "="*250)
            self.logger.info("CRITIC ANALYSIS COMPLETE")
            self.logger.info("="*250)
            
            return AgentOutput(
                success=True,
                content={
                    "draft_id": agent_input.metadata.get('draft_id', 'unknown'),
                    "critiques": critiques,
                    "overall_assessment": overall_assessment,
                    "critique_summary": {
                        "total_issues": len(critiques),
                        "critical_issues": len([c for c in critiques if c.get('severity') == 'critical']),
                        "high_issues": len([c for c in critiques if c.get('severity') == 'high']),
                        "verification_categories": {
                            "logical": len([c for c in critiques if c.get('type') == 'logic_flaw']),
                            "factual": len([c for c in critiques if c.get('type') == 'fact_contradiction']),
                            "hallucination": len([c for c in critiques if c.get('type') == 'hallucination'])
                        }
                    }
                },
                metadata={
                    "verification_scope": {
                        "cot_steps_analyzed": len(chain_of_thought),
                        "context_items_checked": len(context),
                        "draft_length": len(draft_content)
                    }
                },
                processing_time=0.0,  # Set by parent class
                agent_role=self.agent_role
            )
            
        except Exception as e:
            self.logger.error(f"Critic failed: {str(e)}")
            raise e
    
    async def _verify_logical_consistency(
        self, 
        chain_of_thought: List[Dict[str, Any]], 
        draft_content: str
    ) -> List[Dict[str, Any]]:
        """Verify logical flow and consistency in reasoning"""
        
        logical_issues = []
        
        if not chain_of_thought:
            logical_issues.append({
                "type": "logic_flaw",
                "severity": "high",
                "description": "No Chain-of-Thought provided for verification",
                "step_ref": None
            })
            return logical_issues
        
        try:
            # Check each reasoning step for logical validity
            for i, step in enumerate(chain_of_thought):
                step_issues = await self._analyze_reasoning_step(step, i, chain_of_thought)
                logical_issues.extend(step_issues)
            
            # Check overall logical flow
            flow_issues = await self._analyze_logical_flow(chain_of_thought, draft_content)
            logical_issues.extend(flow_issues)
            
            self.logger.debug(f"Logical verification found {len(logical_issues)} issues")
            
        except Exception as e:
            self.logger.error(f"Logical verification failed: {str(e)}")
            logical_issues.append({
                "type": "logic_flaw",
                "severity": "medium",
                "description": f"Logical verification failed due to error: {str(e)}",
                "step_ref": None
            })
        
        return logical_issues
    
    async def _analyze_reasoning_step(
        self, 
        step: Dict[str, Any], 
        step_index: int, 
        all_steps: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Analyze a single reasoning step for logical issues"""
        
        issues = []
        step_num = step.get('step', step_index + 1)
        step_thought = step.get('thought', '')
        
        # Check for empty or trivial steps
        if len(step_thought.strip()) < 20:
            issues.append({
                "type": "logic_flaw",
                "severity": "medium",
                "step_ref": step_num,
                "description": f"Step {step_num} is too brief or lacks substance"
            })
            return issues
        
        # Use LLM to analyze logical validity
        if self.llm_client:
            try:
                previous_context = ""
                if step_index > 0:
                    prev_steps = all_steps[:step_index]
                    previous_context = "\n".join([f"Step {s.get('step', i+1)}: {s.get('thought', '')}" for i, s in enumerate(prev_steps)])
                
                prompt = f"""
                Analyze the logical validity of the following reasoning step:

                Previous steps context:
                {previous_context}

                Current step to analyze:
                Step {step_num}: {step_thought}

                Check for:
                1. Logical fallacies or invalid inferences
                2. Unsupported leaps in reasoning
                3. Contradictions with previous steps
                4. Missing crucial logical connections

                If you find issues, respond in this format:
                ISSUE: [brief description]
                SEVERITY: [critical/high/medium/low]
                EXPLANATION: [detailed explanation]

                If no significant logical issues, respond: "NO_ISSUES"
                """
                
                response = await self._call_llm(prompt, temperature=0.1)
                
                # ULTRA VERBOSE: Log evaluation decision
                self.logger.info(f"LOGICAL STEP {step_num} EVALUATION:")
                if response and "NO_ISSUES" not in response.upper():
                    self.logger.info(f"ISSUES DETECTED in step {step_num}")
                    # Parse LLM response for issues
                    parsed_issues = self._parse_llm_critique(response, step_num, "logic_flaw")
                    self.logger.info(f"PARSED {len(parsed_issues)} logical issues from response")
                    for issue in parsed_issues:
                        self.logger.info(f"   • {issue.get('severity', 'unknown').upper()}: {issue.get('description', 'no description')}")
                    issues.extend(parsed_issues)
                else:
                    self.logger.info(f"NO LOGICAL ISSUES found in step {step_num}")
                    
            except Exception as e:
                self.logger.warning(f"️ LLM-based step analysis failed: {str(e)}")
        
        return issues
    
    async def _analyze_logical_flow(
        self, 
        chain_of_thought: List[Dict[str, Any]], 
        draft_content: str
    ) -> List[Dict[str, Any]]:
        """Analyze overall logical flow from CoT to draft"""
        
        issues = []
        
        if not self.llm_client:
            return issues
        
        try:
            # Create summary of reasoning chain
            cot_summary = "\n".join([
                f"Step {step.get('step', i+1)}: {step.get('thought', '')}"
                for i, step in enumerate(chain_of_thought)
            ])
            
            prompt = f"""
            Analyze the logical flow from reasoning steps to final draft:

======= REASONING CHAIN START =======
{cot_summary}
======= REASONING CHAIN END =======

======= FINAL DRAFT START =======
{draft_content}
======= FINAL DRAFT END =======

            Check for:
            1. Does the draft logically follow from the reasoning chain?
            2. Are there major gaps between reasoning and conclusions?
            3. Does the draft contradict any reasoning steps?
            4. Are key reasoning insights missing from the draft?

            If you find significant flow issues, respond in this format:
            ISSUE: [brief description]
            SEVERITY: [critical/high/medium/low]
            EXPLANATION: [detailed explanation]

            If the flow is generally sound, respond: "NO_MAJOR_ISSUES"
            """
            
            # ULTRA VERBOSE: Log the full prompt being sent to LLM
            self.logger.info(f"\n" + "="*250)
            self.logger.info(f"LOGICAL FLOW ANALYSIS - FULL PROMPT TO LLM")
            self.logger.info(f"="*250)
            self.logger.info(f"PROMPT CONTENT:")
            self.logger.info(f"{prompt}")
            self.logger.info(f"="*250)
            
            response = await self._call_llm(prompt, temperature=0.1)
            
            self.logger.info(f"\nLLM RESPONSE FOR LOGICAL FLOW:")
            self.logger.info(f"{response}")
            self.logger.info(f"="*250)
            
            # ULTRA VERBOSE: Log flow evaluation decision
            self.logger.info(f"LOGICAL FLOW EVALUATION:")
            if response and "NO_MAJOR_ISSUES" not in response.upper():
                self.logger.info(f"LOGICAL FLOW ISSUES DETECTED")
                parsed_issues = self._parse_llm_critique(response, None, "logic_flaw")
                self.logger.info(f"PARSED {len(parsed_issues)} flow issues from response")
                for issue in parsed_issues:
                    self.logger.info(f"   • {issue.get('severity', 'unknown').upper()}: {issue.get('description', 'no description')}")
                issues.extend(parsed_issues)
            else:
                self.logger.info(f"LOGICAL FLOW IS SOUND")
                
        except Exception as e:
            self.logger.warning(f"️ Logical flow analysis failed: {str(e)}")
        
        return issues
    
    async def _verify_factual_accuracy(
        self, 
        draft_content: str, 
        context: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Verify factual claims against provided context"""
        
        factual_issues = []
        
        if not context:
            self.logger.info("No context provided for fact-checking")
            return factual_issues
        
        try:
            # Extract key factual claims from draft
            claims = self._extract_factual_claims(draft_content)
            
            # ULTRA VERBOSE: Log extracted claims
            self.logger.info(f"FACTUAL CLAIMS EXTRACTION:")
            self.logger.info(f"EXTRACTED {len(claims)} factual claims from draft:")
            for i, claim in enumerate(claims, 1):
                self.logger.info(f"   {i}. '{claim}'")
            
            if not claims:
                self.logger.info("️ NO EXPLICIT FACTUAL CLAIMS FOUND for verification")
                return factual_issues
            
            # Verify each claim against context
            for claim in claims:
                verification_result = await self._verify_single_claim(claim, context)
                if verification_result:
                    factual_issues.append(verification_result)
            
            self.logger.debug(f"Fact-checking found {len(factual_issues)} issues")
            
        except Exception as e:
            self.logger.error(f"Factual verification failed: {str(e)}")
            factual_issues.append({
                "type": "fact_contradiction",
                "severity": "medium",
                "description": f"Fact-checking failed due to error: {str(e)}",
                "claim": None
            })
        
        return factual_issues
    
    def _extract_factual_claims(self, content: str) -> List[str]:
        """Extract specific factual claims from content"""
        claims = []
        
        # Look for specific patterns that indicate factual claims
        patterns = [
            r"The value is (\d+\.?\d*)",
            r"The result is (\w+)",
            r"According to (.+?),",
            r"The formula is (.+?)[\.\n]",
            r"(\w+) equals (\w+)",
            r"The answer is (.+?)[\.\n]"
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            for match in matches:
                if isinstance(match, tuple):
                    claim = " ".join(match).strip()
                else:
                    claim = match.strip()
                
                if len(claim) > 5:  # Filter out very short matches
                    claims.append(claim)
        
        # Also extract sentences with definitive statements
        sentences = re.split(r'[.!?]+', content)
        for sentence in sentences:
            sentence = sentence.strip()
            if any(indicator in sentence.lower() for indicator in ['is', 'equals', 'equals to', 'the value', 'the result']):
                if 20 < len(sentence) < 200:  # Reasonable length
                    claims.append(sentence)
        
        return list(set(claims))  # Remove duplicates
    
    async def _verify_single_claim(
        self, 
        claim: str, 
        context: List[Dict[str, Any]]
    ) -> Dict[str, Any] | None:
        """Verify a single claim against context"""
        
        if not self.llm_client:
            return None
        
        try:
            # Create context summary for verification
            context_text = "\n".join([
                f"Source {i+1}: {item.get('text', item.get('content', ''))}"
                for i, item in enumerate(context[:5])  # Limit context to prevent overflow
            ])
            
            # ULTRA VERBOSE: Show exactly what context is being used for verification
            self.logger.info(f"\n--- FACT-CHECKING CLAIM AGAINST CONTEXT ---")
            self.logger.info(f"Claim: '{claim}'")
            self.logger.info(f"Context sources ({len(context)} total, showing first 5):")
            self.logger.info(f"'{context_text}'")
            
            prompt = f"""
            Verify the following claim against the provided context:

======= CLAIM TO VERIFY START =======
{claim}
======= CLAIM TO VERIFY END =======

======= CONTEXT SOURCES START =======
{context_text}
======= CONTEXT SOURCES END =======

            Determine:
            1. Is this claim explicitly supported by the context?
            2. Is this claim contradicted by the context?
            3. Is this claim not mentioned in the context at all?

            Respond in this format:
            VERIFICATION: [SUPPORTED/CONTRADICTED/NOT_MENTIONED]
            EVIDENCE: [specific quote from context if applicable]
            CONFIDENCE: [high/medium/low]

            If CONTRADICTED, also include:
            SEVERITY: [critical/high/medium/low]
            """
            
            # ULTRA VERBOSE: Log the full fact-check prompt
            self.logger.info(f"\n" + "="*250)
            self.logger.info(f"FACT-CHECK ANALYSIS - FULL PROMPT TO LLM")
            self.logger.info(f"="*250)
            self.logger.info(f"CLAIM: '{claim}'")
            self.logger.info(f"PROMPT CONTENT:")
            self.logger.info(f"{prompt}")
            self.logger.info(f"="*250)
            
            response = await self._call_llm(prompt, temperature=0.1)
            
            self.logger.info(f"\nLLM RESPONSE FOR FACT-CHECK:")
            self.logger.info(f"{response}")
            self.logger.info(f"="*250)
            
            # ULTRA VERBOSE: Log fact-check evaluation
            self.logger.info(f"FACT-CHECK EVALUATION for claim: '{claim}'")
            if response and "CONTRADICTED" in response.upper():
                self.logger.info(f"FACTUAL CONTRADICTION DETECTED")
                # Parse the contradiction details
                severity = "high"  # Default
                evidence = ""
                
                lines = response.split('\n')
                for line in lines:
                    if line.startswith('EVIDENCE:'):
                        evidence = line[9:].strip()
                    elif line.startswith('SEVERITY:'):
                        severity = line[9:].strip().lower()
                
                self.logger.info(f"CONTRADICTION: Severity={severity.upper()}, Evidence='{evidence}'")
                return {
                    "type": "fact_contradiction",
                    "severity": severity,
                    "claim": claim,
                    "description": f"Claim contradicted by provided context: {evidence}"
                }
            else:
                self.logger.info(f"FACT-CHECK PASSED for claim")
            
        except Exception as e:
            self.logger.warning(f"️ Single claim verification failed: {str(e)}")
        
        return None
    
    async def _detect_hallucinations(
        self, 
        draft_content: str, 
        context: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """Detect hallucinated information not supported by context"""
        
        hallucination_issues = []
        
        if not self.llm_client or not context:
            return hallucination_issues
        
        try:
            # Create comprehensive context summary
            context_summary = self._create_context_summary(context)
            
            # ULTRA VERBOSE: Show exactly what's being checked for hallucinations
            self.logger.info(f"\n--- HALLUCINATION DETECTION INPUT ---")
            self.logger.info(f"Draft content ({len(draft_content)} chars): '{draft_content}'")
            self.logger.info(f"Context summary ({len(context_summary)} chars): '{context_summary}'")
            
            prompt = f"""
            Analyze the draft for potential hallucinations - information that appears to be made up or not supported by the provided context.

            DRAFT CONTENT:
            {draft_content}

            AVAILABLE CONTEXT:
            {context_summary}

            Look for:
            1. Specific facts, figures, or formulas not in the context
            2. References to concepts not mentioned in the context  
            3. Made-up examples or case studies
            4. Invented technical terms or jargon
            5. Fabricated historical details or citations

            For each potential hallucination, respond in this format:
            HALLUCINATION: [specific content that appears fabricated]
            SEVERITY: [critical/high/medium/low]
            EXPLANATION: [why this appears to be hallucinated]

            If no clear hallucinations are detected, respond: "NO_HALLUCINATIONS_DETECTED"
            """
            
            response = await self._call_llm(prompt, temperature=0.1)
            
            # ULTRA VERBOSE: Log hallucination evaluation
            self.logger.info(f"HALLUCINATION DETECTION EVALUATION:")
            # Check if response contains actual hallucination reports (not just the "no hallucinations" phrase)
            has_severity = "SEVERITY:" in response.upper() if response else False
            has_hallucination_tag = "HALLUCINATION:" in response.upper() if response else False
            
            if response and (has_severity or has_hallucination_tag):
                self.logger.info(f"HALLUCINATIONS DETECTED")
                parsed_issues = self._parse_llm_critique(response, None, "hallucination")
                self.logger.info(f"PARSED {len(parsed_issues)} hallucination issues from response")
                for issue in parsed_issues:
                    self.logger.info(f"   • {issue.get('severity', 'unknown').upper()}: {issue.get('description', 'no description')}")
                hallucination_issues.extend(parsed_issues)
            else:
                self.logger.info(f"NO HALLUCINATIONS DETECTED")
            
            self.logger.debug(f"Hallucination detection found {len(hallucination_issues)} issues")
            
        except Exception as e:
            self.logger.error(f"Hallucination detection failed: {str(e)}")
        
        return hallucination_issues
    
    def _create_context_summary(self, context: List[Dict[str, Any]]) -> str:
        """Create a summary of available context for hallucination detection"""
        if not context:
            return "No context provided."
        
        summaries = []
        for i, item in enumerate(context[:8]):  # Limit to prevent prompt overflow
            text = item.get('text', item.get('content', ''))
            score = item.get('score', 'N/A')
            
            summary = f"Context {i+1} (Relevance: {score}):\n{text}"
            summaries.append(summary)
        
        return "\n\n".join(summaries)
    
    def _parse_llm_critique(
        self, 
        response: str, 
        step_ref: int | None, 
        issue_type: str
    ) -> List[Dict[str, Any]]:
        """Parse LLM response into structured critique format"""
        
        issues = []
        
        try:
            # ULTRA VERBOSE: Log what we're trying to parse
            self.logger.info(f"PARSING LLM RESPONSE:")
            self.logger.info(f"Response length: {len(response)} chars")
            self.logger.info(f"Looking for SEVERITY: {('SEVERITY:' in response.upper())}")
            self.logger.info(f"Looking for HALLUCINATION: {('HALLUCINATION:' in response.upper())}")
            
            # Handle different response formats
            if "SEVERITY:" in response.upper():
                # Format: explanatory text with SEVERITY: embedded
                issue = {"type": issue_type}
                
                if step_ref is not None:
                    issue["step_ref"] = step_ref
                
                lines = response.strip().split('\n')
                description_parts = []
                
                for line in lines:
                    line = line.strip()
                    
                    if line.startswith('ISSUE:') or line.startswith('HALLUCINATION:'):
                        description_parts.append(line.split(':', 1)[1].strip())
                    elif line.startswith('SEVERITY:'):
                        severity = line[9:].strip().lower()
                        if severity in self.severity_levels:
                            issue["severity"] = severity
                        else:
                            issue["severity"] = "medium"
                        self.logger.info(f"Found severity: {severity}")
                    elif line.startswith('EXPLANATION:'):
                        explanation = line[12:].strip()
                        description_parts.append(explanation)
                    elif line and not line.startswith('CONFIDENCE:') and len(line) > 20:
                        # Include substantial descriptive lines
                        description_parts.append(line)
                
                # Combine description parts
                if description_parts:
                    issue["description"] = " - ".join(description_parts[:2])  # Limit to first 2 parts
                else:
                    issue["description"] = "Issue detected but description unclear"
                
                if not issue.get("severity"):
                    issue["severity"] = "medium"  # Default
                    
                issues.append(issue)
                self.logger.info(f"Created issue: {issue.get('severity', 'unknown').upper()} - {issue.get('description', '')}")
            
            else:
                # Original format: Split response into individual issues
                issue_blocks = re.split(r'\n(?=ISSUE:|HALLUCINATION:)', response)
                
                for block in issue_blocks:
                    if not block.strip():
                        continue
                    
                    issue = {"type": issue_type}
                    
                    if step_ref is not None:
                        issue["step_ref"] = step_ref
                    
                    lines = block.strip().split('\n')
                    for line in lines:
                        line = line.strip()
                        
                        if line.startswith('ISSUE:') or line.startswith('HALLUCINATION:'):
                            issue["description"] = line.split(':', 1)[1].strip()
                        elif line.startswith('SEVERITY:'):
                            severity = line[9:].strip().lower()
                            if severity in self.severity_levels:
                                issue["severity"] = severity
                            else:
                                issue["severity"] = "medium"  # Default
                        elif line.startswith('EXPLANATION:'):
                            explanation = line[12:].strip()
                            if "description" in issue:
                                issue["description"] += f" - {explanation}"
                            else:
                                issue["description"] = explanation
                    
                    # Only add issues with valid descriptions
                    if "description" in issue:
                        issues.append(issue)
            
        except Exception as e:
            self.logger.warning(f"️ Failed to parse LLM critique: {str(e)}")
            
            # Fallback: create a single issue from raw response
            if response and len(response) > 10:
                issues.append({
                    "type": issue_type,
                    "severity": "medium",
                    "description": response,
                    "step_ref": step_ref
                })
        
        return issues
    
    def _assess_overall_quality(self, critiques: List[Dict[str, Any]]) -> str:
        """Assess overall quality based on critique severity"""
        
        if not critiques:
            return "acceptable"
        
        # Count issues by severity
        critical_count = len([c for c in critiques if c.get('severity') == 'critical'])
        high_count = len([c for c in critiques if c.get('severity') == 'high'])
        
        if critical_count > 0:
            return "major_revisions_required"
        elif high_count > 2:
            return "significant_revisions_required" 
        elif high_count > 0 or len(critiques) > 3:
            return "minor_revisions_suggested"
        else:
            return "acceptable_with_minor_issues"
    
    async def _call_llm(self, prompt: str, temperature: float) -> str:
        """Call LLM with error handling and ultra-verbose debugging"""
        
        # ULTRA VERBOSE: Log the exact prompt being sent
        self.logger.info("\n" + "="*250)
        self.logger.info("CRITIC LLM CALL START")
        self.logger.info("="*250)
        self.logger.info(f"Temperature: {temperature}")
        self.logger.info(f"\nPROMPT:")
        self.logger.info("-"*250)
        self.logger.info(prompt)
        self.logger.info("-"*250)
        
        try:
            if hasattr(self.llm_client, 'get_llm_client'):
                llm = self.llm_client.get_llm_client()
                response = await llm.ainvoke(prompt, temperature=temperature)
                response_text = response.content if hasattr(response, 'content') else str(response)
            else:
                # Fallback for different LLM client interfaces
                response = await self.llm_client.generate(prompt, temperature=temperature)
                response_text = str(response)
            
            # ULTRA VERBOSE: Log the exact response received
            self.logger.info(f"\nLLM RESPONSE:")
            self.logger.info("-"*250)
            self.logger.info(response_text)
            self.logger.info("-"*250)
            self.logger.info("\n" + "="*250)
            self.logger.info("CRITIC LLM CALL COMPLETE")
            self.logger.info("="*250)
            
            return response_text
            
        except Exception as e:
            self.logger.error(f"LLM call FAILED: {str(e)}")
            self.logger.info("=== CRITIC LLM CALL FAILED ===")
            return "" 

🔹 FILE: ./agents/reporter_agent.py
-----------------------------------
"""
Reporter Agent - Report Writer

This agent synthesizes the final answer after debate convergence, creating
polished, refined, and pedagogically valuable responses.
"""

from typing import List, Dict, Any
from ai_agents.agents.base_agent import BaseAgent, AgentInput, AgentOutput, AgentRole


class ReporterAgent(BaseAgent):
    """
    Reporter Agent - Final Answer Synthesizer
    
    Responsible for:
    1. Synthesizing verified draft into final polished answer
    2. Handling both converged and deadlock scenarios
    3. Formatting answers for educational value
    4. Providing source attribution and citations
    """
    
    def __init__(self, config, llm_client=None, logger=None):
        super().__init__(AgentRole.REPORTER, config, llm_client, logger)
        
        # Reporter-specific prompts
        self.system_prompt = self._build_system_prompt()
        
    def _build_system_prompt(self) -> str:
        """Build the system prompt for the reporter"""
        return """
        You are an expert educational content writer and report synthesizer. Your role is to:

        1. SYNTHESIZE verified content into polished, final answers
        2. STRUCTURE responses for maximum educational value
        3. INTEGRATE remaining minor issues seamlessly
        4. ATTRIBUTE sources clearly and transparently
        5. MAINTAIN academic rigor while ensuring accessibility
        6. **CLEAN AND FIX** all retrieved content formatting issues

        Key principles:
        - Write in the tone of a seasoned, knowledgeable teacher
        - Organize content logically: introduction, steps, key takeaways
        - **Fix all document parsing artifacts**: broken sentences, missing punctuation, fragmented text
        - **Transform raw retrieval text** into coherent, well-structured explanations
        - **Ensure proper grammar and flow** - don't copy-paste raw retrieved content
        - **Present information naturally** - don't mention "Document 1", "sources", or "retrieved materials"
        - Be transparent about knowledge boundaries and limitations
        - Provide clear, actionable insights
        - Ensure content is suitable for educational contexts
        - Reflect carefully on the debate outcome before finalizing

        CRITICAL: The retrieved content may have formatting issues, incomplete sentences, broken mathematical expressions, or parsing errors. 
        Your job is to understand the meaning and rewrite it as clear, professional educational content that flows naturally without referencing sources.
        
        **SPECIAL ATTENTION TO MATH**: Fix incomplete formulas (e.g., "f(x) = x^" → "f(x) = x^n"), integrate scattered mathematical symbols, and ensure all equations are properly formatted and complete.

        Your output should be the definitive, high-quality answer to the user's question.
        """
    
    async def process(self, agent_input: AgentInput) -> AgentOutput:
        """
        Synthesize final answer from debate results
        
        Args:
            agent_input: Contains draft, critique results, and convergence status
            
        Returns:
            AgentOutput: Final polished answer ready for user
        """
        try:
            # Extract debate results
            draft_content = agent_input.metadata.get('draft_content', '')
            chain_of_thought = agent_input.metadata.get('chain_of_thought', [])
            final_draft_status = agent_input.metadata.get('final_draft_status', {})
            remaining_critiques = agent_input.metadata.get('remaining_critiques', [])
            context = agent_input.context
            original_query = agent_input.query
            
            debate_status = final_draft_status.get('status', 'approved')
            quality_score = final_draft_status.get('quality_score', 0.8)
            
            self.logger.info(f"Reporter synthesizing final answer...")
            self.logger.info(f"Debate status: {debate_status}, Quality: {quality_score:.3f}")
            
            # Generate final answer based on debate outcome
            if debate_status == "approved":
                final_answer = await self._synthesize_approved_answer(
                    original_query, draft_content, chain_of_thought, remaining_critiques, context
                )
            elif debate_status == "escalated":
                quality_warning = final_draft_status.get('quality_warning', 'Quality issues detected after multiple improvement attempts')
                final_answer = await self._synthesize_escalated_answer(
                    original_query, draft_content, remaining_critiques, context, quality_warning
                )
            elif debate_status == "deadlock":
                final_answer = await self._synthesize_deadlock_answer(
                    original_query, draft_content, remaining_critiques, context
                )
            else:
                # Fallback for unexpected status
                final_answer = await self._synthesize_fallback_answer(
                    original_query, draft_content, context
                )
            
            # Enhance with metadata and sources
            enhanced_answer = self._enhance_with_metadata(
                final_answer, context, quality_score, debate_status
            )
            
            return AgentOutput(
                success=True,
                content={
                    "final_answer": enhanced_answer,
                    "synthesis_metadata": {
                        "debate_status": debate_status,
                        "quality_score": quality_score,
                        "remaining_issues": len(remaining_critiques or []),
                        "context_sources": len(context),
                        "answer_structure": {
                            "has_introduction": "introduction" in enhanced_answer,
                            "has_step_by_step": "step_by_step_solution" in enhanced_answer,
                            "has_takeaways": "key_takeaways" in enhanced_answer,
                            "has_sources": "sources" in enhanced_answer
                        }
                    }
                },
                metadata={
                    "original_query": original_query,
                    "synthesis_approach": debate_status,
                    "educational_format": True
                },
                processing_time=0.0,  # Set by parent class
                agent_role=self.agent_role
            )
            
        except Exception as e:
            self.logger.error(f"Reporter failed: {str(e)}")
            raise e
    
    async def _synthesize_approved_answer(
        self, 
        query: str, 
        draft_content: str, 
        chain_of_thought: List[Dict[str, Any]], 
        remaining_critiques: List[Dict[str, Any]], 
        context: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Synthesize final answer from approved draft"""
        
        try:
            # Build comprehensive synthesis prompt
            cot_summary = self._format_chain_of_thought_summary(chain_of_thought)
            minor_issues = self._format_minor_issues(remaining_critiques)
            context_summary = self._format_context_summary(context)
            
            prompt = f"""
            {self.system_prompt}
            
            ORIGINAL QUERY:
            {query}
            
            VERIFIED DRAFT CONTENT:
            {draft_content}
            
            REASONING PROCESS:
            {cot_summary}
            
            MINOR REMAINING ISSUES TO ADDRESS:
            {minor_issues}
            
            SUPPORTING CONTEXT:
            {context_summary}
            
            Please synthesize this into a final, polished answer using this structure:
            
            ## INTRODUCTION
            [Brief context-setting introduction that acknowledges the question and previews the approach]
            
            ## STEP-BY-STEP SOLUTION
            [Clear, logical progression through the solution, incorporating insights from the reasoning process]
            
            ## KEY TAKEAWAYS
            [Important concepts, principles, or insights that generalize beyond this specific question]
            
            ## IMPORTANT NOTES
            [Any limitations, assumptions, or areas requiring caution - address minor issues transparently]
            
            Requirements:
            - **CLEAN UP FORMATTING ISSUES**: Fix broken sentences, missing punctuation, fragmented text from document parsing
            - **FORMAT MATH FOR KATEX**: Use proper LaTeX syntax - inline math: $f(x) = x^2$, display math: $$f(x) = x^2$$
            - **FIX MATHEMATICAL EXPRESSIONS ONLY**: Complete broken formulas (e.g., "f(x) = x^" → "$f(x) = x^2$"), integrate scattered math symbols with $ delimiters
            - **NO LONE MATH SYMBOLS**: Never leave symbols like π on separate lines - integrate them into complete sentences or expressions
            - **COMPLETE ALL BROKEN FORMULAS**: Fix incomplete expressions and fragmented mathematical content
            - **FIX SENTENCE FRAGMENTS**: Combine broken text pieces into complete, flowing sentences, remove trailing "and" or incomplete endings
            - **SYNTHESIZE CONCISELY**: Transform raw retrieved content into coherent explanations without unnecessary expansion
            - **FIX DOCUMENT PARSING ARTIFACTS**: Remove formatting errors, incomplete sentences, and garbled text
            - **CREATE PROPER FLOW**: Ensure logical transitions between concepts and ideas
            - **USE ACADEMIC WRITING STYLE**: Clear, professional, and educational tone
            - **NO DOCUMENT REFERENCES**: Don't mention "Document 1", "according to sources", or "based on provided materials" - present information naturally
            - **KEEP ORIGINAL SCOPE**: Don't expand beyond the original content's scope unless necessary for clarity
            - Integrate minor issues seamlessly (don't ignore them, but address them naturally)
            - Maintain educational value and clear explanations
            - Use a confident but honest tone
            - Ensure accuracy and logical flow
            """
            
            response = await self._call_llm(prompt, temperature=0.3)
            
            if response:
                return self._parse_structured_answer(response)
            else:
                raise Exception("No response from LLM for answer synthesis")
                
        except Exception as e:
            self.logger.error(f"Approved answer synthesis failed: {str(e)}")
            # Fallback to basic structure
            return self._create_fallback_structure(draft_content, query)
    
    async def _synthesize_escalated_answer(
        self, 
        query: str, 
        draft_content: str, 
        persistent_critiques: List[Dict[str, Any]], 
        context: List[Dict[str, Any]],
        quality_warning: str
    ) -> Dict[str, Any]:
        """Synthesize answer for escalated situation with quality warnings"""
        
        try:
            critiques_summary = self._format_unresolved_issues(persistent_critiques)
            context_summary = self._format_context_summary(context)
            
            prompt = f"""
            {self.system_prompt}
            
            SITUATION: After 3 improvement iterations, some quality issues remain unresolved. You need to provide the best possible answer while clearly warning the user about potential limitations.
            
            ORIGINAL QUERY:
            {query}
            
            BEST AVAILABLE DRAFT:
            {draft_content}
            
            PERSISTENT QUALITY ISSUES:
            {critiques_summary}
            
            QUALITY WARNING TO INCLUDE:
            {quality_warning}
            
            SUPPORTING CONTEXT:
            {context_summary}
            
            INSTRUCTIONS:
            1. Provide a comprehensive answer based on the available draft and context
            2. PROMINENTLY include the quality warning at the beginning
            3. Clearly indicate which parts may be uncertain or problematic
            4. Suggest ways the user can verify or improve the information
            5. Maintain educational value while being transparent about limitations
            
            Format your response as a well-structured educational answer with clear sections.
            """
            
            response = await self._call_llm(prompt, temperature=0.3)
            
            return {
                "content": response,
                "status": "escalated",
                "quality_warning": quality_warning,
                "persistent_issues": len(persistent_critiques),
                "transparency": "This answer includes quality warnings due to unresolved issues after multiple improvement attempts."
            }
            
        except Exception as e:
            self.logger.error(f"Escalated answer synthesis failed: {e}")
            return {
                "content": f"I apologize, but I encountered difficulties providing a complete answer to your question. The system detected quality issues that couldn't be fully resolved: {quality_warning}",
                "status": "error",
                "quality_warning": quality_warning
            }
    
    async def _synthesize_deadlock_answer(
        self, 
        query: str, 
        draft_content: str, 
        unresolved_critiques: List[Dict[str, Any]], 
        context: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Synthesize answer for deadlock situation with transparency"""
        
        try:
            unresolved_summary = self._format_unresolved_issues(unresolved_critiques)
            context_summary = self._format_context_summary(context)
            
            prompt = f"""
            {self.system_prompt}
            
            SITUATION: The debate process reached a deadlock without full convergence. You need to provide the best possible answer while being transparent about limitations.
            
            ORIGINAL QUERY:
            {query}
            
            BEST AVAILABLE DRAFT:
            {draft_content}
            
            UNRESOLVED ISSUES:
            {unresolved_summary}
            
            SUPPORTING CONTEXT:
            {context_summary}
            
            Please create a transparent, educational response using this structure:
            
            ## PARTIAL SOLUTION
            [Present the best available information and reasoning, clearly indicating confidence levels]
            
            ## AREAS OF UNCERTAINTY
            [Honestly discuss unresolved aspects, conflicting information, or gaps in knowledge]
            
            ## WHAT WE CAN CONCLUDE
            [Clearly state what can be confidently concluded from available information]
            
            ## RECOMMENDATIONS FOR FURTHER EXPLORATION
            [Suggest specific areas for additional research or verification]
            
            Requirements:
            - Be completely honest about limitations
            - Still provide maximum educational value
            - Maintain academic integrity
            - Guide user toward reliable sources for unclear areas
            """
            
            response = await self._call_llm(prompt, temperature=0.2)
            
            if response:
                return self._parse_structured_answer(response, deadlock_mode=True)
            else:
                raise Exception("No response from LLM for deadlock synthesis")
                
        except Exception as e:
            self.logger.error(f"Deadlock answer synthesis failed: {str(e)}")
            # Fallback with transparency message
            return {
                "partial_solution": draft_content or "Unable to provide complete solution due to unresolved issues.",
                "areas_of_uncertainty": "Multiple technical issues prevented full verification of this response.",
                "recommendations": "Please consult additional authoritative sources for verification."
            }
    
    async def _synthesize_fallback_answer(
        self, 
        query: str, 
        draft_content: str, 
        context: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Synthesize basic answer as fallback"""
        
        return {
            "introduction": f"In response to your query: {query}",
            "step_by_step_solution": draft_content or "Unable to generate complete solution.",
            "key_takeaways": "Additional analysis would be needed for complete insights.",
            "important_notes": "This response may require further verification."
        }
    
    def _format_chain_of_thought_summary(self, chain_of_thought: List[Dict[str, Any]]) -> str:
        """Format Chain of Thought for synthesis prompt"""
        
        if not chain_of_thought:
            return "No detailed reasoning process available."
        
        formatted_steps = []
        for step in chain_of_thought:
            step_num = step.get('step', 'N/A')
            thought = step.get('thought', '')
            details = step.get('details', [])
            
            formatted_step = f"Step {step_num}: {thought}"
            if details:
                formatted_step += f"\n  - {'; '.join(details[:3])}"  # Limit details
            
            formatted_steps.append(formatted_step)
        
        return "\n".join(formatted_steps)
    
    def _format_minor_issues(self, critiques: List[Dict[str, Any]]) -> str:
        """Format minor remaining issues for synthesis"""
        
        if not critiques:
            return "No minor issues to address."
        
        issue_descriptions = []
        for critique in critiques[:5]:  # Limit to most important
            severity = critique.get('severity', 'unknown')
            description = critique.get('description', 'No description')
            issue_type = critique.get('type', 'unknown')
            
            issue_descriptions.append(f"• ({severity}) {description}")
        
        return f"Minor issues to integrate naturally:\n" + "\n".join(issue_descriptions)
    
    def _format_unresolved_issues(self, critiques: List[Dict[str, Any]]) -> str:
        """Format unresolved issues for deadlock transparency"""
        
        if not critiques:
            return "No specific unresolved issues documented."
        
        # Group by severity for clear presentation
        by_severity = {}
        for critique in critiques:
            severity = critique.get('severity', 'medium')
            if severity not in by_severity:
                by_severity[severity] = []
            by_severity[severity].append(critique.get('description', 'No description'))
        
        formatted_issues = []
        for severity in ['critical', 'high', 'medium', 'low']:
            if severity in by_severity:
                issues = by_severity[severity][:3]  # Limit per severity
                formatted_issues.append(f"{severity.upper()} ISSUES:")
                for issue in issues:
                    formatted_issues.append(f"• {issue}")
        
        return "\n".join(formatted_issues)
    
    def _format_context_summary(self, context: List[Dict[str, Any]]) -> str:
        """Format context sources for synthesis"""
        
        if not context:
            return "No additional context sources available."
        
        summaries = []
        for i, ctx_item in enumerate(context[:3]):  # Limit to top sources
            text = ctx_item.get('text', ctx_item.get('content', ''))
            score = ctx_item.get('score', 'N/A')
            source = ctx_item.get('source', {})
            
            summary = f"Source {i+1} (Relevance: {score}):\n{text[:300]}..."
            summaries.append(summary)
        
        return "\n\n".join(summaries)
    
    def _parse_structured_answer(self, response: str, deadlock_mode: bool = False) -> Dict[str, Any]:
        """Parse LLM response into structured answer format"""
        
        answer = {}
        
        try:
            # Split response into sections
            sections = {}
            current_section = None
            current_content = []
            
            for line in response.split('\n'):
                line = line.strip()
                
                if line.startswith('## '):
                    # Save previous section
                    if current_section:
                        sections[current_section] = '\n'.join(current_content)
                    
                    # Start new section
                    current_section = line[3:].strip().lower().replace(' ', '_')
                    current_content = []
                    
                elif current_section and line:
                    current_content.append(line)
            
            # Save last section
            if current_section:
                sections[current_section] = '\n'.join(current_content)
            
            # Map sections to answer structure
            if deadlock_mode:
                answer["partial_solution"] = sections.get('partial_solution', response)
                answer["areas_of_uncertainty"] = sections.get('areas_of_uncertainty', '')
                answer["what_we_can_conclude"] = sections.get('what_we_can_conclude', '')
                answer["recommendations_for_further_exploration"] = sections.get('recommendations_for_further_exploration', '')
            else:
                answer["introduction"] = sections.get('introduction', '')
                answer["step_by_step_solution"] = sections.get('step-by-step_solution', sections.get('step_by_step_solution', response))
                answer["key_takeaways"] = sections.get('key_takeaways', '')
                answer["important_notes"] = sections.get('important_notes', '')
            
        except Exception as e:
            self.logger.warning(f"️ Answer parsing failed, using raw response: {str(e)}")
            # Fallback to raw response
            if deadlock_mode:
                answer["partial_solution"] = response
            else:
                answer["step_by_step_solution"] = response
        
        return answer
    
    def _create_fallback_structure(self, content: str, query: str) -> Dict[str, Any]:
        """Create basic fallback structure"""
        return {
            "introduction": f"Addressing your question: {query}",
            "step_by_step_solution": content or "Unable to generate complete solution.",
            "key_takeaways": "This response was generated with limited verification.",
            "important_notes": "Please verify this information with additional sources."
        }
    
    def _enhance_with_metadata(
        self, 
        answer: Dict[str, Any], 
        context: List[Dict[str, Any]], 
        quality_score: float, 
        debate_status: str
    ) -> Dict[str, Any]:
        """Enhance answer with confidence score, sources, and metadata"""
        
        enhanced = answer.copy()
        
        # Add confidence score
        enhanced["confidence_score"] = quality_score
        
        # Add source attribution
        sources = []
        for ctx_item in context[:5]:  # Limit sources
            source_info = ctx_item.get('source', {})
            score = ctx_item.get('score', 'N/A')
            
            if isinstance(source_info, dict):
                # Extract meaningful source information
                source_id = source_info.get('document_id', source_info.get('course_id', 'Unknown'))
                sources.append(f"{source_id} (relevance: {score})")
            else:
                sources.append(str(source_info))
        
        enhanced["sources"] = sources
        
        # Add quality indicators
        enhanced["quality_indicators"] = {
            "debate_status": debate_status,
            "verification_level": "high" if quality_score > 0.8 else "medium" if quality_score > 0.5 else "limited",
            "context_support": "strong" if len(context) >= 3 else "moderate" if len(context) >= 1 else "limited"
        }
        
        return enhanced
    
    async def _call_llm(self, prompt: str, temperature: float) -> str:
        """Call LLM with error handling"""
        try:
            if hasattr(self.llm_client, 'get_llm_client'):
                llm = self.llm_client.get_llm_client()
                response = await llm.ainvoke(prompt, temperature=temperature)
                content = response.content if hasattr(response, 'content') else str(response)
                
                # DEBUG: Log what the agents system generates
                print("=== DEBUG AGENT SYSTEM OUTPUT ===")
                print("AGENT LLM RESPONSE:", repr(content))  # Full content
                print("================================")
                
                return content
            else:
                # Fallback for different LLM client interfaces
                response = await self.llm_client.generate(prompt, temperature=temperature)
                content_str = str(response)
                
                # DEBUG: Log what the agents system generates
                print("=== DEBUG AGENT SYSTEM OUTPUT ===")
                print("AGENT LLM RESPONSE:", repr(content_str))  # Full content
                print("================================")
                
                return content_str
        except Exception as e:
            self.logger.error(f"LLM call failed: {str(e)}")
            return "" 

🔹 FILE: ./README.md
-----------------------------------
# Speculative AI Multi-Agent System

A sophisticated multi-agent reasoning system that enhances traditional RAG with debate-based verification and iterative improvement.

## Architecture Overview

The system consists of 5 specialized agents working in concert:

1. **Retrieve Agent** - Speculative retriever with query reframing
2. **Strategist Agent** - Draft solution generator with Chain-of-Thought reasoning  
3. **Critic Agent** - Critical verification and issue identification
4. **Moderator Agent** - Debate flow control and convergence decisions
5. **Reporter Agent** - Final answer synthesis and formatting

## Key Features

- **Speculative Query Reframing** - Automatically generates alternative queries when initial retrieval quality is poor
- **Multi-Round Debate Process** - Iterative improvement through critic feedback
- **Convergence Detection** - Intelligent stopping criteria based on critique severity
- **Transparent Quality Assessment** - Clear indicators of verification level and confidence
- **Educational Formatting** - Structured answers optimized for learning

## Usage

### Via REST API

```bash
# Start the service (included in main setup.sh)
cd machine_learning/speculative_ai
uvicorn app.main:app --reload --host 0.0.0.0 --port 8003

# Query the system
curl -X POST "http://localhost:8003/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Explain Lagrange multipliers in optimization",
    "course_id": "math_optimization",
    "session_id": "demo_session"
  }'
```

### Via Backend Integration

The system is integrated with the main backend. Select "speculative" as the model in the chat interface.

```python
# In chat interface, the system is called automatically when:
data = ChatRequest(
    prompt="Your question here",
    model="speculative",  # Key parameter
    course_id="your_course_id"
)
```

## Configuration

Key configuration options in `SpeculativeAIConfig`:

```python
config = SpeculativeAIConfig(
    max_debate_rounds=3,           # Maximum iteration rounds
    retrieval_k=10,                # Initial retrieval count
    speculation_rounds=3,          # Alternative query generation
    convergence_threshold=0.7,     # Quality threshold for convergence
    enable_debug_logging=True      # Detailed debug information
)
```

## Response Format

### Standard Response (Converged)
```json
{
  "success": true,
  "answer": {
    "introduction": "Brief context-setting introduction",
    "step_by_step_solution": "Detailed solution with reasoning",
    "key_takeaways": "Important concepts and insights",
    "important_notes": "Limitations and considerations",
    "confidence_score": 0.95,
    "sources": ["source1.pdf", "source2.pdf"],
    "quality_indicators": {
      "debate_status": "converged",
      "verification_level": "high",
      "context_support": "strong"
    }
  },
  "metadata": {
    "debate_rounds": 2,
    "convergence_score": 0.95,
    "processing_time": 12.5
  }
}
```

### Deadlock Response (Partial)
```json
{
  "success": true,
  "answer": {
    "partial_solution": "Best available information",
    "areas_of_uncertainty": "Unresolved aspects",
    "what_we_can_conclude": "Confident conclusions",
    "recommendations_for_further_exploration": "Suggested research directions"
  }
}
```

## Quality Indicators

- **Verification Level**: high | medium | limited
- **Context Support**: strong | moderate | limited  
- **Debate Status**: converged | deadlock
- **Convergence Score**: 0.0-1.0 (confidence in final answer)

## Performance Metrics

The system tracks comprehensive performance metrics:

- **Convergence Rate**: Percentage of queries that reach satisfactory conclusions
- **Average Debate Rounds**: Mean number of iterations per query
- **Deadlock Rate**: Percentage of queries that reach maximum rounds without convergence
- **Agent Performance**: Individual agent execution times and success rates

## Dependencies

Core dependencies are shared with the parent RAG system:

- FastAPI for REST API
- LangChain for LLM orchestration
- Google Gemini for language model
- Pydantic for data validation
- asyncio for concurrent processing

## Development

### Running Tests
```bash
# Run the integrated system test
python -m pytest tests/ -v

# Test individual agents
python -m pytest tests/test_agents.py -v
```

### Adding New Agents

1. Create agent class inheriting from `BaseAgent`
2. Implement the `process()` method
3. Register with `AgentRegistry` in orchestrator
4. Update orchestrator workflow logic

### Configuration Tuning

Key parameters to adjust based on use case:

- `max_debate_rounds`: Higher for complex domains, lower for speed
- `retrieval_k`: More for comprehensive coverage, fewer for efficiency  
- `convergence_threshold`: Higher for stricter quality, lower for faster responses
- `speculation_rounds`: More for difficult queries, fewer for efficiency

## Integration Points

The system integrates with:

1. **RAG System** (port 8002) - For document retrieval
2. **Backend Chat** (port 8000) - Via model="speculative"
3. **LLM Services** - Gemini, OpenAI, Anthropic (configurable)

## Monitoring

Monitor system health via:

```bash
# System status
curl http://localhost:8003/status

# Agent health  
curl http://localhost:8003/health/agents

# Configuration (debug mode only)
curl http://localhost:8003/debug/config
```

## Troubleshooting

Common issues:

1. **Service Unavailable (503)**: RAG system or LLM client not initialized
2. **No Context Found**: Ensure course_id exists in vector database  
3. **Debate Deadlock**: Check critique severity thresholds in config
4. **High Processing Time**: Reduce max_debate_rounds or retrieval_k

## Architecture Principles

The system follows key principles from the requirements:

- **Pragmatic Pattern Usage**: Only applies complexity where it adds clear value
- **Single Responsibility**: Each agent has one clear purpose
- **Composition over Inheritance**: Agents are composed, not derived
- **Rule of Three**: Abstractions only after proven need across agents
- **Clean Separation**: Business logic separate from configuration

This ensures the system remains maintainable while providing advanced reasoning capabilities. 

🔹 FILE: ./orchestrator.py
-----------------------------------
"""
Speculative AI Orchestrator

Main orchestrator that coordinates all agents in the multi-agent reasoning system.
Implements the complete debate loop with proper error handling and monitoring.
"""

import asyncio
import logging
from typing import Dict, Any, Optional, List, AsyncGenerator
from datetime import datetime

from ai_agents.config import SpeculativeAIConfig
from ai_agents.agents.base_agent import AgentRole, AgentInput, AgentOutput
from ai_agents.agents.retrieve_agent import RetrieveAgent
from ai_agents.agents.strategist_agent import StrategistAgent
from ai_agents.agents.critic_agent import CriticAgent
from ai_agents.agents.moderator_agent import ModeratorAgent
from ai_agents.agents.reporter_agent import ReporterAgent
from ai_agents.agents.tutor_agent import TutorAgent
from rag_system.llm_clients.cerebras_client import CerebrasClient
from rag_system.llm_clients.gemini_client import GeminiClient


class MultiAgentOrchestrator:
    """
    Main orchestrator for the Multi-Agent System
    
    Coordinates the complete workflow:
    1. Retrieve - Enhanced retrieval with query reframing
    2. Strategist - Generate draft solution with CoT
    3. Critic - Critical verification and issue identification
    4. Moderator - Debate flow control and convergence decisions
    5. Reporter - Final answer synthesis and formatting
    """
    
    def __init__(
        self, 
        config: Optional[SpeculativeAIConfig] = None,
        rag_service=None,
        llm_client=None,
        logger: Optional[logging.Logger] = None
    ):
        self.config = config or SpeculativeAIConfig()
        self.rag_service = rag_service
        self.llm_client = llm_client
        self.logger = logger or logging.getLogger("ai_agents.orchestrator")
        
        # Initialize agents directly
        self._setup_agents()
        
        # Session tracking
        self.current_sessions = {}
        
        # Agent conversation tracker
        self.conversation_history = []
        
        # Performance metrics
        self.execution_stats = {
            "total_queries": 0,
            "successful_completions": 0,
            "convergence_rate": 0.0,
            "average_debate_rounds": 0.0,
            "deadlock_rate": 0.0
        }
        
        self.logger.info("Multi-Agent System Orchestrator initialized")
        self.logger.info(f"Config: max_rounds={self.config.max_debate_rounds}, retrieval_k={self.config.retrieval_k}")
    def _setup_agents(self):
        """Initialize all agents directly"""
        
        # Create agent instances with shared configuration
        self.retrieve_agent = RetrieveAgent(
            config=self.config,
            llm_client=self.llm_client,
            rag_service=self.rag_service,
            logger=self.logger.getChild("retrieve")
        )
        
        self.strategist_agent = StrategistAgent(
            config=self.config,
            llm_client=self.llm_client,
            logger=self.logger.getChild("strategist")
        )
        
        self.critic_agent = CriticAgent(
            config=self.config,
            llm_client=self.llm_client,
            logger=self.logger.getChild("critic")
        )
        
        self.moderator_agent = ModeratorAgent(
            config=self.config,
            llm_client=self.llm_client,
            logger=self.logger.getChild("moderator")
        )
        
        self.reporter_agent = ReporterAgent(
            config=self.config,
            llm_client=self.llm_client,
            logger=self.logger.getChild("reporter")
        )
        
        self.tutor_agent = TutorAgent(
            config=self.config,
            llm_client=self.llm_client,
            logger=self.logger.getChild("tutor")
        )
        
        self.logger.info(f"Initialized 6 agents (Retrieve, Strategist, Critic, Moderator, Reporter, Tutor)")

    def _create_llm_client(self, model_name: str):
        """Create an LLM client based on model name."""
        try:
            if model_name.startswith("gemini"):
                return GeminiClient(
                    api_key=self.rag_service.settings.google_api_key,
                    model=model_name,
                    temperature=0.6,
                )
            if model_name.startswith("qwen") or model_name.startswith("cerebras"):
                return CerebrasClient(
                    api_key=self.rag_service.settings.cerebras_api_key,
                    model=model_name,
                )
        except Exception as e:
            self.logger.error(f"Failed to create llm client for {model_name}: {e}")
        return None
    
    def _log_agent_conversation(self, agent_name: str, input_data: Any, output_data: Any, stage: str = ""):
        """Log agent conversation in detailed chat-group format"""
        timestamp = datetime.now().strftime("%H:%M:%S")
        
        # Enhanced input formatting
        if hasattr(input_data, 'query'):
            query_preview = input_data.query[:100] + "..." if len(input_data.query) > 100 else input_data.query
            input_preview = f"'{query_preview}'"
        elif isinstance(input_data, dict):
            input_preview = f"Data: {str(input_data)[:80]}..."
        else:
            input_preview = f"Input: {str(input_data)[:80]}..."
        
        # Enhanced output formatting with detailed content
        if hasattr(output_data, 'content') and output_data.content:
            if 'draft_content' in output_data.content:
                draft = output_data.content['draft_content']
                draft_preview = draft[:300] + "..." if len(draft) > 300 else draft
                output_preview = f"Generated draft ({len(draft)} chars):\n{draft_preview}"
                
            elif 'critiques' in output_data.content:
                critiques = output_data.content['critiques'] or []
                if critiques:
                    critique_details = []
                    for i, critique in enumerate(critiques[:2], 1):
                        issue = critique.get('issue', 'Unknown issue')[:80]
                        critique_details.append(f"  • {issue}")
                    critique_text = "\n".join(critique_details)
                    if len(critiques) > 2:
                        critique_text += f"\n  • (+{len(critiques)-2} more issues)"
                    output_preview = f"Found {len(critiques)} issues:\n{critique_text}"
                else:
                    output_preview = "No issues found - draft approved!"
                    
            elif 'decision' in output_data.content:
                decision = output_data.content['decision']
                reasoning = output_data.content.get('reasoning', '')
                reasoning_preview = reasoning[:120] + "..." if len(reasoning) > 120 else reasoning
                output_preview = f"Decision: {decision}\n  Reasoning: {reasoning_preview}"
                
            elif 'final_answer' in output_data.content:
                answer = str(output_data.content['final_answer'])
                if isinstance(output_data.content['final_answer'], dict):
                    # Extract key parts of structured answer
                    intro = output_data.content['final_answer'].get('introduction', '')
                    solution = output_data.content['final_answer'].get('step_by_step_solution', '')
                    answer_preview = f"Introduction: {intro}\nSolution: {solution}"
                else:
                    answer_preview = answer
                output_preview = f"Final answer:\n{answer_preview}"
                
            elif 'retrieval_results' in output_data.content:
                results = output_data.content['retrieval_results']
                quality = output_data.content.get('quality_assessment', {}).get('score', 0)
                output_preview = f"Retrieved {len(results)} chunks (quality: {quality:.3f})"
                if results:
                    for i, result in enumerate(results[:2], 1):
                        content = result.get('content', '')
                        score = result.get('score', 'N/A')
                        output_preview += f"\n  • Chunk {i}: {content} (score: {score})"
                    if len(results) > 2:
                        output_preview += f"\n  • (+{len(results)-2} more chunks)"
                        
            else:
                content_str = str(output_data.content)
                output_preview = content_str
        else:
            output_preview = str(output_data)
        
        # Add to conversation history
        conversation_entry = {
            "timestamp": timestamp,
            "agent": agent_name,
            "stage": stage,
            "input": input_preview,
            "output": output_preview,
            "success": getattr(output_data, 'success', True)
        }
        
        if not hasattr(self, 'conversation_history'):
            self.conversation_history = []
        self.conversation_history.append(conversation_entry)
        
        # Display as chat group conversation
        status_icon = "SUCCESS" if conversation_entry["success"] else "ERROR"
        stage_info = f" [{stage}]" if stage else ""
        
        self.logger.info(f"")
        self.logger.info(f"=== {agent_name.upper()}{stage_info} @ {timestamp} ===")
        self.logger.info(f"INPUT: {input_preview}")
        self.logger.info(f"OUTPUT [{status_icon}]: {output_preview}")
        
        # Add processing time if available
        if hasattr(output_data, 'processing_time') and output_data.processing_time > 0:
            self.logger.info(f"PROCESSING TIME: {output_data.processing_time:.2f}s")
    
    def _display_conversation_summary(self):
        """Display the full agent conversation like a chat history"""
        if not hasattr(self, 'conversation_history') or not self.conversation_history:
            self.logger.info("AGENT CONVERSATION HISTORY: No conversation data available")
            return
            
        self.logger.info("AGENT CONVERSATION HISTORY:")
        self.logger.info("=" * 80)
        
        for entry in self.conversation_history:
            status = "" if entry["success"] else ""
            stage_info = f" ({entry['stage']})" if entry['stage'] else ""
            
            self.logger.info(f"[{entry['timestamp']}] {entry['agent']}{stage_info}:")
            self.logger.info(f"  {entry['input']}")
            self.logger.info(f"  {status} {entry['output']}")
            self.logger.info("")
        
        self.logger.info("=" * 80)
    
    async def process_query(
        self,
        query: str,
        course_id: str,
        session_id: str,
        metadata: Optional[Dict[str, Any]] = None,
        heavy_model: Optional[str] = None,
        course_prompt: Optional[str] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Process a user query through the complete speculative AI workflow
        """
        
        start_time = datetime.now()
        self.execution_stats["total_queries"] += 1

        # Clear conversation history for new query
        self.conversation_history = []

        heavy_llm = None
        original_strategist_llm = None
        original_critic_llm = None

        try:
            self.logger.info(f"QUERY: '{query[:80]}...' | Course: {course_id[:8]}...")
            
            yield {"status": "in_progress", "stage": "initialization", "message": "Starting agent processing..."}
            
            # Add course prompt to metadata for all agents
            enhanced_metadata = metadata.copy() if metadata else {}
            if course_prompt:
                enhanced_metadata['course_prompt'] = course_prompt
                self.logger.info(f"Using course-specific prompt: {course_prompt[:50]}...")
            
            # Use heavy model if specified, otherwise fall back to base model from metadata
            debate_model = heavy_model
            if not debate_model:
                # If no heavy model specified, check metadata for base model
                debate_model = enhanced_metadata.get('base_model')
            
            if debate_model:
                debate_llm = self._create_llm_client(debate_model)
                if debate_llm:
                    original_strategist_llm = self.strategist_agent.llm_client
                    original_critic_llm = self.critic_agent.llm_client
                    self.strategist_agent.llm_client = debate_llm
                    self.critic_agent.llm_client = debate_llm
                    self.logger.info(f"Using {debate_model} for debate agents")
            
            # Stage 1: Enhanced Retrieval
            self.logger.info("")
            self.logger.info("=== RETRIEVAL STAGE ===")
            yield {"status": "in_progress", "stage": "retrieval", "message": "Performing contextual retrieval..."}
            retrieval_result = await self._execute_retrieval(query, course_id, session_id, enhanced_metadata)
            
            if not retrieval_result.success:
                yield self._create_error_response("Retrieval failed", retrieval_result.error_message)
                return # Stop execution on error

            context = retrieval_result.content.get("retrieval_results", [])
            self.logger.info(f"   Retrieved {len(context)} context chunks")
            yield {"status": "in_progress", "stage": "retrieval_complete", "message": f"Retrieved {len(context)} context chunks.", "context_items": len(context)}

            # Stage 2: Debate Loop
            self.logger.info("")
            self.logger.info("️  === DEBATE STAGE ===")
            yield {"status": "in_progress", "stage": "debate", "message": "Starting multi-agent debate..."}
            debate_result = await self._execute_debate_loop(query, context, session_id, enhanced_metadata)
            
            if not debate_result["success"]:
                yield self._create_error_response("Debate loop failed", debate_result.get("error"))
                return # Stop execution on error
            
            yield {"status": "in_progress", "stage": "debate_complete", "message": f"Debate completed in {debate_result['result']['debate_rounds']} rounds.", "debate_rounds": debate_result['result']['debate_rounds']}
            
            # Stage 3: Final Synthesis
            self.logger.info("")
            self.logger.info("=== SYNTHESIS STAGE ===")
            yield {"status": "in_progress", "stage": "synthesis", "message": "Synthesizing final answer..."}
            final_result = await self._execute_final_synthesis(
                query, context, debate_result["result"], session_id, enhanced_metadata
            )
            
            if not final_result.success:
                yield self._create_error_response("Final synthesis failed", final_result.error_message)
                return # Stop execution on error
            
            yield {"status": "in_progress", "stage": "synthesis_complete", "message": "Final answer synthesized."}
            
            # Stage 4: Tutor Interaction
            self.logger.info("")
            self.logger.info("=== TUTOR STAGE ===")
            yield {"status": "in_progress", "stage": "tutor_interaction", "message": "Engaging tutor for additional insights..."}
            tutor_result = await self._execute_tutor_interaction(
                query, final_result.content["final_answer"], session_id, enhanced_metadata
            )
            
            if not tutor_result.success:
                self.logger.warning(f"Tutor interaction failed: {tutor_result.error_message}")
                # Continue with basic response if tutor fails
                tutor_content = {"interaction_type": "basic", "elements": [{"type": "answer", "content": final_result.content["final_answer"]}]}
            else:
                tutor_content = tutor_result.content
            
            # Update success metrics
            processing_time = (datetime.now() - start_time).total_seconds()
            self.execution_stats["successful_completions"] += 1
            self._update_execution_stats(debate_result["result"])
            
            # Show conversation summary
            self._display_conversation_summary()
            
            # Format final response with tutor interaction
            response = self._format_tutor_response(
                tutor_content,
                final_result.content["final_answer"],
                retrieval_result,
                debate_result["result"],
                final_result,
                processing_time
            )
            
            self.logger.info(f"Query completed successfully in {processing_time:.2f}s")
            yield {"status": "complete", "final_response": response, "processing_time": processing_time}

        except Exception as e:
            self.logger.error(f"Query processing failed: {str(e)}")
            yield self._create_error_response("System error", str(e))
        finally:
            if heavy_llm:
                if original_strategist_llm:
                    self.strategist_agent.llm_client = original_strategist_llm
                if original_critic_llm:
                    self.critic_agent.llm_client = original_critic_llm
    
    async def _execute_retrieval(
        self, 
        query: str, 
        course_id: str, 
        session_id: str, 
        metadata: Optional[Dict[str, Any]]
    ) -> AgentOutput:
        """Execute enhanced retrieval stage"""
        
        retrieve_agent = self.retrieve_agent
        
        if not retrieve_agent:
            self.logger.error("Retrieve agent not available!")
            raise Exception("Retrieve agent not available")
        
        retrieval_input = AgentInput(
            query=query,
            context=[],  # No initial context for retrieval
            metadata={
                "course_id": course_id,
                **(metadata or {})
            },
            session_id=session_id
        )
        
        result = await retrieve_agent.execute(retrieval_input)
        
        # Log the agent conversation
        self._log_agent_conversation("Retrieve", retrieval_input, result, "Retrieval")
        
        return result
    
    async def _execute_debate_loop(
        self, 
        query: str, 
        context: List[Dict[str, Any]], 
        session_id: str, 
        metadata: Optional[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """Execute the complete debate loop with iteration control"""
        
        self.logger.info(f"")
        self.logger.info(f"[DEBATE LOOP] Starting multi-agent debate")
        self.logger.info(f"   Context items: {len(context)}")
        self.logger.info(f"   Max rounds: {self.config.max_debate_rounds}")
        
        try:
            current_round = 1
            current_draft = None
            current_cot = None
            
            # Get agents
            strategist = self.strategist_agent
            critic = self.critic_agent
            moderator = self.moderator_agent
            
            if not all([strategist, critic, moderator]):
                self.logger.error("[DEBATE LOOP] Required debate agents not available!")
                raise Exception("Required debate agents not available")
            
            while current_round <= self.config.max_debate_rounds:
                self.logger.info(f"")
                self.logger.info(f"=== ROUND {current_round}/{self.config.max_debate_rounds} ===")
                
                # Stage 1: Strategist generates draft and CoT
                self.logger.info(f"STRATEGIST: Analyzing query and generating draft...")
                strategist_input = AgentInput(
                    query=query,
                    context=context,
                    metadata={
                        "round": current_round,
                        "previous_feedback": getattr(self, '_last_feedback', None),
                        **(metadata or {})
                    },
                    session_id=session_id
                )
                
                strategist_result = await strategist.execute(strategist_input)
                
                # Log the agent conversation
                self._log_agent_conversation("Strategist", strategist_input, strategist_result, f"Round {current_round}")
                
                if not strategist_result.success:
                    return {"success": False, "error": f"Strategist failed in round {current_round}"}
                
                current_draft = strategist_result.content["draft_content"]
                current_cot = strategist_result.content["chain_of_thought"]
                draft_id = strategist_result.content["draft_id"]

                # Step 2: Critic analyzes draft
                self.logger.info(f"CRITIC: Evaluating draft for issues...")
                critic_input = AgentInput(
                    query=query,
                    context=context,
                    metadata={
                        "draft_content": current_draft,
                        "chain_of_thought": current_cot,
                        "draft_id": draft_id,
                        "round": current_round,
                        **(metadata or {})
                    },
                    session_id=session_id
                )
                
                critic_result = await critic.execute(critic_input)
                
                # Log the agent conversation
                self._log_agent_conversation("Critic", critic_input, critic_result, f"Round {current_round}")
                
                if not critic_result.success:
                    return {"success": False, "error": f"Critic failed in round {current_round}"}
                
                critiques = critic_result.content["critiques"]
                overall_assessment = critic_result.content["overall_assessment"]

                # Step 3: Moderator decides next action
                self.logger.info(f"MODERATOR: Making decision on draft quality...")
                moderator_input = AgentInput(
                    query=query,
                    context=context,
                    metadata={
                        "critiques": critiques,
                        "draft_id": draft_id,
                        "current_round": current_round,
                        "overall_assessment": overall_assessment,
                        "draft_content": current_draft,
                        "chain_of_thought": current_cot,
                        **(metadata or {})
                    },
                    session_id=session_id
                )
                
                moderator_result = await moderator.execute(moderator_input)
                
                # Log the agent conversation
                self._log_agent_conversation("Moderator", moderator_input, moderator_result, f"Round {current_round}")
                
                if not moderator_result.success:
                    return {"success": False, "error": f"Moderator failed in round {current_round}"}
                
                decision = moderator_result.content["decision"]
                
                if decision not in ["converged", "iterate", "abort_deadlock", "escalate_with_warning"]:
                    self.logger.warning(f"Unknown decision: {decision}")
                
                # Act on moderator decision
                if decision == "converged":
                    self.logger.info(f"   Debate converged after {current_round} rounds")
                    return {
                        "success": True,
                        "result": {
                            "status": "converged",
                            "final_draft": {
                                "content": current_draft,
                                "cot": current_cot,
                                "draft_id": draft_id,
                                "status": "approved",
                                "quality_score": moderator_result.content["decision_metadata"]["convergence_score"]
                            },
                            "remaining_critiques": moderator_result.content.get("critiques") or [],
                            "debate_rounds": current_round,
                            "convergence_score": moderator_result.content["decision_metadata"]["convergence_score"]
                        }
                    }
                
                elif decision == "abort_deadlock":
                    self.logger.info(f"   ️ Debate deadlocked after {current_round} rounds")
                    return {
                        "success": True,
                        "result": {
                            "status": "deadlock",
                            "final_draft": {
                                "content": current_draft,
                                "cot": current_cot,
                                "draft_id": draft_id,
                                "status": "deadlock",
                                "quality_score": moderator_result.content["decision_metadata"]["convergence_score"]
                            },
                            "remaining_critiques": critiques or [],
                            "debate_rounds": current_round,
                            "convergence_score": moderator_result.content["decision_metadata"]["convergence_score"]
                        }
                    }
                
                elif decision == "escalate_with_warning":
                    self.logger.info(f"️ Quality issues escalated after {current_round} rounds")
                    return {
                        "success": True,
                        "result": {
                            "status": "escalated",
                            "final_draft": {
                                "content": current_draft,
                                "cot": current_cot,
                                "draft_id": draft_id,
                                "status": "escalated",
                                "quality_score": moderator_result.content["decision_metadata"]["convergence_score"],
                                "quality_warning": moderator_result.content.get("warning_message", "Quality issues detected")
                            },
                            "remaining_critiques": critiques or [],
                            "debate_rounds": current_round,
                            "convergence_score": moderator_result.content["decision_metadata"]["convergence_score"],
                            "escalation_warning": moderator_result.content.get("warning_message")
                        }
                    }
                
                elif decision == "iterate":
                    # Prepare for next iteration
                    feedback = moderator_result.content.get("feedback_to_strategist", "")
                    self.logger.debug(f"Moderator feedback type: {type(feedback)}, content: {feedback}")
                    self._last_feedback = feedback or ""
                    current_round += 1
                
                else:
                    return {"success": False, "error": f"Unknown moderator decision: {decision}"}
            
            # Should not reach here due to moderator deadlock detection
            return {"success": False, "error": "Debate loop exceeded maximum rounds"}
            
        except Exception as e:
            self.logger.error(f"Debate loop failed: {str(e)}")
            return {"success": False, "error": str(e)}
    
    async def _execute_final_synthesis(
        self, 
        query: str, 
        context: List[Dict[str, Any]], 
        debate_result: Dict[str, Any], 
        session_id: str, 
        metadata: Optional[Dict[str, Any]]
    ) -> AgentOutput:
        """Execute final answer synthesis"""
        
        self.logger.info("[REPORTER] Starting final synthesis")
        
        reporter = self.reporter_agent
        
        if not reporter:
            self.logger.error("[REPORTER] Agent not available!")
            raise Exception("Reporter agent not available")
        
        final_draft = debate_result["final_draft"]
        self.logger.info(f"[REPORTER] Processing final draft with status: {final_draft.get('status', 'unknown')}")
        
        reporter_input = AgentInput(
            query=query,
            context=context,
            metadata={
                "draft_content": final_draft["content"],
                "chain_of_thought": final_draft["cot"],
                "final_draft_status": {
                    "status": final_draft["status"],
                    "quality_score": final_draft["quality_score"]
                },
                "remaining_critiques": debate_result.get("remaining_critiques", []),
                "debate_rounds": debate_result["debate_rounds"],
                **(metadata or {})
            },
            session_id=session_id
        )
        
        result = await reporter.execute(reporter_input)
        
        # Log the agent conversation
        self._log_agent_conversation("Reporter", reporter_input, result, "Synthesis")
        
        return result
    
    async def _execute_tutor_interaction(
        self,
        query: str,
        final_answer: Dict[str, Any],
        session_id: str,
        metadata: Optional[Dict[str, Any]]
    ) -> AgentOutput:
        """Execute tutor interaction stage"""
        
        tutor_input = AgentInput(
            query=query,
            context=[],  # Tutor doesn't need retrieval context
            metadata={
                "final_answer": final_answer,
                "conversation_history": metadata.get("conversation_history", []),
                **(metadata or {})
            },
            session_id=session_id
        )
        
        result = await self.tutor_agent.execute(tutor_input)
        
        # Log the agent conversation
        self._log_agent_conversation("Tutor", tutor_input, result, "Interaction")
        
        return result
    
    def _format_final_response(
        self, 
        final_answer: Dict[str, Any], 
        retrieval_result: AgentOutput, 
        debate_result: Dict[str, Any], 
        synthesis_result: AgentOutput,
        processing_time: float
    ) -> Dict[str, Any]:
        """Format the complete response for the user"""
        
        return {
            "success": True,
            "answer": final_answer,
            "metadata": {
                "processing_time": processing_time,
                "debate_status": debate_result["status"],
                "debate_rounds": debate_result["debate_rounds"],
                "convergence_score": debate_result["convergence_score"],
                "retrieval_quality": retrieval_result.content.get("quality_assessment", {}),
                "synthesis_metadata": synthesis_result.content.get("synthesis_metadata", {}),
                "agent_performance": self._get_simple_metrics()
            },
            "debug_info": {
                "retrieval_strategy": retrieval_result.metadata.get("retrieval_strategy"),
                "context_items": len(retrieval_result.content.get("retrieval_results", [])),
                "remaining_issues": len(debate_result.get("remaining_critiques", [])),
                "quality_indicators": final_answer.get("quality_indicators", {})
            } if self.config.enable_debug_logging else {}
        }
    
    def _format_tutor_response(
        self,
        tutor_content: Dict[str, Any],
        final_answer: Dict[str, Any],
        retrieval_result: AgentOutput,
        debate_result: Dict[str, Any],
        synthesis_result: AgentOutput,
        processing_time: float
    ) -> Dict[str, Any]:
        """Format response with tutor interaction"""
        
        return {
            "success": True,
            "answer": final_answer,
            "tutor_interaction": tutor_content,
            "metadata": {
                "processing_time": processing_time,
                "debate_status": debate_result["status"],
                "debate_rounds": debate_result["debate_rounds"],
                "convergence_score": debate_result["convergence_score"],
                "retrieval_quality": retrieval_result.content.get("quality_assessment", {}),
                "synthesis_metadata": synthesis_result.content.get("synthesis_metadata", {}),
                "agent_performance": self._get_simple_metrics()
            },
            "debug_info": {
                "retrieval_strategy": retrieval_result.metadata.get("retrieval_strategy"),
                "context_items": len(retrieval_result.content.get("retrieval_results", [])),
                "remaining_issues": len(debate_result.get("remaining_critiques", [])),
                "quality_indicators": final_answer.get("quality_indicators", {})
            } if self.config.enable_debug_logging else {}
        }
    
    def _create_error_response(self, error_type: str, error_message: str) -> Dict[str, Any]:
        """Create standardized error response"""
        return {
            "success": False,
            "error": {
                "type": error_type,
                "message": error_message,
                "timestamp": datetime.now().isoformat()
            },
                            "error_response": {
                "introduction": "I apologize, but I encountered an error while processing your query.",
                "step_by_step_solution": f"Error: {error_message}",
                "important_notes": "Please try rephrasing your question or contact support if the issue persists."
            }
        }
    
    def _update_execution_stats(self, debate_result: Dict[str, Any]):
        """Update execution statistics"""
        total_completions = self.execution_stats["successful_completions"]
        
        # Update convergence rate
        if debate_result["status"] == "converged":
            converged_count = getattr(self, '_converged_count', 0) + 1
            self._converged_count = converged_count
            self.execution_stats["convergence_rate"] = converged_count / total_completions
        
        # Update deadlock rate
        if debate_result["status"] == "deadlock":
            deadlock_count = getattr(self, '_deadlock_count', 0) + 1
            self._deadlock_count = deadlock_count
            self.execution_stats["deadlock_rate"] = deadlock_count / total_completions
        
        # Update average debate rounds
        total_rounds = getattr(self, '_total_rounds', 0) + debate_result["debate_rounds"]
        self._total_rounds = total_rounds
        self.execution_stats["average_debate_rounds"] = total_rounds / total_completions
    
    def get_system_status(self) -> Dict[str, Any]:
        """Get comprehensive system status"""
        return {
            "orchestrator_status": "operational",
            "agents_initialized": 6,
            "configuration": {
                "max_debate_rounds": self.config.max_debate_rounds,
                "retrieval_k": self.config.retrieval_k,
                "enable_debug": self.config.enable_debug_logging
            },
            "execution_stats": self.execution_stats,
            "agent_metrics": self._get_simple_metrics()
        }
    
    def _get_simple_metrics(self) -> Dict[str, Any]:
        """Get simple performance metrics for all agents"""
        return {
            "retrieve": self.retrieve_agent.get_metrics(),
            "strategist": self.strategist_agent.get_metrics(),
            "critic": self.critic_agent.get_metrics(),
            "moderator": self.moderator_agent.get_metrics(),
            "reporter": self.reporter_agent.get_metrics(),
            "tutor": self.tutor_agent.get_metrics(),
        }
        
    def reset_system_metrics(self):
        """Reset all system and agent metrics"""
        self.execution_stats = {
            "total_queries": 0,
            "successful_completions": 0,
            "convergence_rate": 0.0,
            "average_debate_rounds": 0.0,
            "deadlock_rate": 0.0
        }
        
        # Reset internal counters
        self._converged_count = 0
        self._deadlock_count = 0
        self._total_rounds = 0
        
        # Reset agent metrics
        for agent in [self.retrieve_agent, self.strategist_agent, self.critic_agent, self.moderator_agent, self.reporter_agent, self.tutor_agent]:
            agent.execution_count = 0
            agent.total_processing_time = 0.0
            agent.error_count = 0
        
        self.logger.info("System metrics reset") 